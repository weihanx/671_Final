{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTY43NIYWvin"
      },
      "source": [
        "# 1. Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BDx6m41hWySX"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "from IPython.display import HTML\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "hCQECPzAWzup"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JBRxsPf9-THY",
        "outputId": "79f831e0-afd3-436a-e106-0456ce86fe6a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "['pathmnist.npz', 'breastmnist.npz', 'pneumoniamnist.npz', 'Evaluating Generative Models for Medical Images.pptx', 'organsmnist.npz', 'bloodmnist.npz', 'Final project.pdf', 'wgan', 'Copy of Conditioned GAN for Medical Image.ipynb', 'wgan_new', 'FID.ipynb', 'Untitled1.ipynb', 'Conditioned GAN for Medical Image.ipynb']\n",
            "google drive = drive/MyDrive/DL_FINAL\n"
          ]
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "from google.colab import drive\n",
        "import os\n",
        "import sys\n",
        "drive.mount(\"/content/drive\")\n",
        "GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = \"DL_FINAL\"\n",
        "GOOGLE_DRIVE_PATH = os.path.join(\"drive\", \"MyDrive\", GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)\n",
        "print(os.listdir(GOOGLE_DRIVE_PATH))\n",
        "\n",
        "# Add to sys so we can import .py files.\n",
        "sys.path.append(GOOGLE_DRIVE_PATH)\n",
        "print(f\"google drive = {GOOGLE_DRIVE_PATH}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfbGSRMLChtq"
      },
      "source": [
        "Check GPU status"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHF9diUGChPt",
        "outputId": "5a49d79e-69c6-4dc7-8c6b-0188293a0c09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Good to go!\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "    print(\"Good to go!\")\n",
        "    DEVICE = torch.device(\"cuda\")\n",
        "else:\n",
        "    print(\"Please set GPU via Edit -> Notebook Settings.\")\n",
        "    DEVICE = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wA5QY9jY9M7J"
      },
      "source": [
        "# 2. Dataset Loading\n",
        "https://github.com/MedMNIST/MedMNIST\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26JUB2CuPH_O"
      },
      "source": [
        "Requirements: one color, two greyscales \\\\\n",
        "\n",
        "Color: PathMNIST\n",
        "\n",
        "Greyscale: PneumoniaMNIST + BreastMNIST\n",
        "\n",
        "\n",
        "Details of the dataset:https://www.nature.com/articles/s41597-022-01721-8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "e6ajSVmjteYU"
      },
      "outputs": [],
      "source": [
        "pathmnist_dir = os.path.join(GOOGLE_DRIVE_PATH, \"pathmnist.npz\")\n",
        "pneumoniamnist_dir = os.path.join(GOOGLE_DRIVE_PATH, \"pneumoniamnist.npz\")\n",
        "breatmnist_dir = os.path.join(GOOGLE_DRIVE_PATH, \"breastmnist.npz\")\n",
        "bloodmnist_dir = os.path.join(GOOGLE_DRIVE_PATH, \"bloodmnist.npz\")\n",
        "organsmnist_dir = os.path.join(GOOGLE_DRIVE_PATH, \"organsmnist.npz\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uA3sAxFp6dAb"
      },
      "source": [
        "#### Custom Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "FKJvfEiA6cXS"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision import transforms\n",
        "import PIL.Image as Image\n",
        "\n",
        "class MedMNISTDataset(Dataset):\n",
        "    def __init__(self, data_dir, split='train', transform=None):\n",
        "        self.data_dir = data_dir\n",
        "        self.transform = transform\n",
        "\n",
        "        # Load data from npz file\n",
        "        data = np.load(data_dir)\n",
        "        if split == 'train':\n",
        "            self.images = data['train_images']\n",
        "            self.labels = data['train_labels']\n",
        "        elif split == 'val':\n",
        "            self.images = data['val_images']\n",
        "            self.labels = data['val_images']\n",
        "        else:\n",
        "          self.images = data['test_images']\n",
        "          self.labels = data['test_labels']\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.fromarray(self.images[idx])\n",
        "        label = int(self.labels[idx])\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmQnA3KhJdF4"
      },
      "source": [
        "#### bloodmnist_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTp8FN-oJb_v",
        "outputId": "affcd13d-4b1c-4c84-fa23-d83e693cfeb7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "len of train_dataset = 11959\n",
            "len of val_dataset = 1712\n",
            "len of test_dataset = 3421\n"
          ]
        }
      ],
      "source": [
        "\n",
        "dataset_dir = bloodmnist_dir\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "\n",
        "train_dataset = MedMNISTDataset(dataset_dir, split='train', transform=transform)\n",
        "val_dataset = MedMNISTDataset(dataset_dir, split='val', transform=transform)\n",
        "test_dataset = MedMNISTDataset(dataset_dir, split='test', transform=transform)\n",
        "\n",
        "\n",
        "bloodmnist_train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "bloodmnist_val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)\n",
        "bloodmnist_test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
        "print(f\"len of train_dataset = {len(train_dataset)}\")\n",
        "print(f\"len of val_dataset = {len(val_dataset)}\")\n",
        "print(f\"len of test_dataset = {len(test_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WTpKIkO3JhIr",
        "outputId": "1aae3999-b3e8-4ec8-adbc-67896b23f77a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([128, 3, 28, 28])\n"
          ]
        }
      ],
      "source": [
        "dataiter = iter(bloodmnist_train_loader)\n",
        "images, labels = next(dataiter)\n",
        "# MNIST image size is 1*28*28\n",
        "img_size = images.shape[2]\n",
        "print(images.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBPUwEz76H8k"
      },
      "source": [
        "#### pneumoniamnist_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3FhCc0nM6HWJ",
        "outputId": "95c99f67-0445-4ac3-fc67-8bbb9e9b1d70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "len of train_dataset = 4708\n",
            "len of val_dataset = 524\n",
            "len of test_dataset = 624\n"
          ]
        }
      ],
      "source": [
        "\n",
        "dataset_dir = pneumoniamnist_dir\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "# Create datasets\n",
        "train_dataset = MedMNISTDataset(dataset_dir, split='train', transform=transform)\n",
        "val_dataset = MedMNISTDataset(dataset_dir, split='val', transform=transform)\n",
        "test_dataset = MedMNISTDataset(dataset_dir, split='test', transform=transform)\n",
        "\n",
        "# Create dataloaders\n",
        "pneu_train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "pneu_val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)\n",
        "pneu_test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
        "print(f\"len of train_dataset = {len(train_dataset)}\")\n",
        "print(f\"len of val_dataset = {len(val_dataset)}\")\n",
        "print(f\"len of test_dataset = {len(test_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LGQfEWJfS9Y9",
        "outputId": "58d916a4-ac60-45eb-edbb-f0be1aa322a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([128, 1, 28, 28])\n"
          ]
        }
      ],
      "source": [
        "dataiter = iter(pneu_train_loader)\n",
        "images, labels = next(dataiter)\n",
        "# MNIST image size is 1*28*28\n",
        "img_size = images.shape[2]\n",
        "print(images.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3e6PqaL6kSd"
      },
      "source": [
        "#### breatmnist_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mjXay0Sa6lc3",
        "outputId": "4053c799-3f70-4c9c-b5be-dd03def3ca78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "len of train_dataset = 546\n",
            "len of val_dataset = 78\n",
            "len of test_dataset = 156\n"
          ]
        }
      ],
      "source": [
        "\n",
        "dataset_dir = breatmnist_dir\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,),(0.5,))\n",
        "])\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = MedMNISTDataset(dataset_dir, split='train', transform=transform)\n",
        "val_dataset = MedMNISTDataset(dataset_dir, split='val', transform=transform)\n",
        "test_dataset = MedMNISTDataset(dataset_dir, split='test', transform=transform)\n",
        "\n",
        "# Create dataloaders\n",
        "breast_train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "breast_val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)\n",
        "breast_test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
        "print(f\"len of train_dataset = {len(train_dataset)}\")\n",
        "print(f\"len of val_dataset = {len(val_dataset)}\")\n",
        "print(f\"len of test_dataset = {len(test_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lfXlIyQTAt6",
        "outputId": "35591492-515c-41d1-ba25-cbe7fcf84f89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([128, 1, 28, 28])\n"
          ]
        }
      ],
      "source": [
        "dataiter = iter(breast_train_loader)\n",
        "images, labels = next(dataiter)\n",
        "# MNIST image size is 1*28*28\n",
        "img_size = images.shape[2]\n",
        "print(images.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxeiNPcXWNOa"
      },
      "source": [
        "# 3. Implement and train a class-conditional GAN on each dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Woi7Ptf98gN_"
      },
      "source": [
        "#### Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "KhVu_LKxWRwL"
      },
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, channels_img, features_d, num_classes, img_size):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.img_size = img_size\n",
        "        self.disc = nn.Sequential(\n",
        "            # input: 128*2*28*28\n",
        "            # ADD ONE MORE CHANNEL\n",
        "            nn.Conv2d(channels_img + 1, features_d, kernel_size=4, stride=2, padding=1), #14*14\n",
        "            nn.LeakyReLU(0.2),\n",
        "            # _block(in_channels, out_channels, kernel_size, stride, padding)\n",
        "            self._block(features_d, features_d * 2, 4, 2, 1), # 7*7\n",
        "            # self._block(features_d*2, features_d *4 , 3, 2, 1), #7*7 16-->\n",
        "            self._block(features_d * 2, features_d * 4, 5, 1, 0), #3*3\n",
        "            # # After all _block img output is 4x4 (Conv2d below makes into 1x1)\n",
        "            nn.Conv2d(features_d * 4, 1, kernel_size=3, stride=1, padding=0)# 1*1\n",
        "        )\n",
        "        self.embed = nn.Embedding(num_classes, img_size * img_size)\n",
        "\n",
        "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels,\n",
        "                out_channels,\n",
        "                kernel_size,\n",
        "                stride,\n",
        "                padding,\n",
        "                bias=False,\n",
        "            ),\n",
        "            nn.InstanceNorm2d(out_channels, affine=True),\n",
        "            nn.LeakyReLU(0.2),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, labels):\n",
        "      # additional channels, reshape to add as one more channel\n",
        "        embedding = self.embed(labels).view(labels.shape[0],1,self.img_size, self.img_size)\n",
        "        # print(f\"embedding shape dis = {embedding.shape}\")\n",
        "        x = torch.cat([x, embedding], dim=1) # N, C+1, H, W\n",
        "        res = self.disc(x)\n",
        "        # print(f\"disrciminator res shape = {res.shape}\")\n",
        "        return res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqv0xTiD8mmf"
      },
      "source": [
        "#### Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "MaO1hXp58l_J"
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, channels_noise, channels_img, features_g, num_classes, img_size, embed_size):\n",
        "        super(Generator, self).__init__()\n",
        "        self.img_size = img_size\n",
        "        self.net = nn.Sequential(\n",
        "            # Input: N x channels_noise x 1 x 1\n",
        "            self._block(channels_noise+embed_size, features_g * 8, 3, 1, 0),  # img: 3*3\n",
        "            self._block(features_g * 8, features_g * 4, 3, 2, 0),  # img: 7*7\n",
        "            self._block(features_g * 4, features_g * 2, 4, 2, 1),  # img: 14*14\n",
        "            # self._block(features_g * 4, features_g * 2, 3, 2, 1),  # img: 28*28\n",
        "            nn.ConvTranspose2d(\n",
        "                features_g * 2, channels_img, kernel_size=4, stride=2, padding=1\n",
        "            ),\n",
        "            # Output: N x channels_img x 28*28\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "        self.embed = nn.Embedding(num_classes, embed_size) # should add at latent dimension\n",
        "\n",
        "\n",
        "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
        "        return nn.Sequential(\n",
        "            nn.ConvTranspose2d(\n",
        "                in_channels,\n",
        "                out_channels,\n",
        "                kernel_size,\n",
        "                stride,\n",
        "                padding,\n",
        "                bias=False,\n",
        "            ),\n",
        "            nn.InstanceNorm2d(out_channels, affine=True),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, labels):\n",
        "      embedding = self.embed(labels).unsqueeze(2).unsqueeze(3) # input: N, embed_size, 1, 1, add dimension.\n",
        "      x = torch.cat([x, embedding], dim=1)\n",
        "\n",
        "      return self.net(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QC_CJ4YpI6Rh"
      },
      "source": [
        "### Weight Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "E5_Q3a5KJCCn"
      },
      "outputs": [],
      "source": [
        "def initialize_weights(model):\n",
        "    # Initializes weights according to the DCGAN paper\n",
        "    for m in model.modules():\n",
        "        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)):\n",
        "            nn.init.normal_(m.weight.data, 0.0, 0.02)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "OnAj0XsDyFIj"
      },
      "outputs": [],
      "source": [
        "# Gradient Penalty:\n",
        "def gradient_penalty(critic, real, fake, labels, device=\"cpu\"):\n",
        "    BATCH_SIZE, C, H, W = real.shape\n",
        "    epsilon = torch.rand((BATCH_SIZE, 1, 1, 1)).repeat(1, C, H, W).to(device)\n",
        "    interpolated_images = real * epsilon + fake * (1 - epsilon) # interpolate\n",
        "\n",
        "    # Calculate critic scores: scores from interpolated images\n",
        "    mixed_scores = critic(interpolated_images, labels)\n",
        "\n",
        "    # Take the gradient of the scores with respect to the images\n",
        "    gradient = torch.autograd.grad(\n",
        "        inputs=interpolated_images,\n",
        "        outputs=mixed_scores,\n",
        "        grad_outputs=torch.ones_like(mixed_scores),\n",
        "        create_graph=True,\n",
        "        retain_graph=True,\n",
        "    )[0] # The first element\n",
        "    gradient = gradient.view(gradient.shape[0], -1)\n",
        "    gradient_norm = gradient.norm(2, dim=1)\n",
        "    gradient_penalty = torch.mean((gradient_norm - 1) ** 2)\n",
        "    return gradient_penalty"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ew6ZPC6sJEDl"
      },
      "source": [
        "### Model Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tzQIDQ06Ui0"
      },
      "source": [
        "### PneumoniaMNIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "id": "0jTga8mgJDkp",
        "outputId": "81e60d99-5176-47f4-bdd8-a2487425b427"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from tqdm import tqdm\n",
        "# Hyperparameters etc.\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "LEARNING_RATE = 1e-4\n",
        "BATCH_SIZE = 64\n",
        "IMG_SIZE = 28\n",
        "CHANNELS_IMG = 1\n",
        "NUM_CLASSES = 2\n",
        "GEN_EMBEDDING = 100\n",
        "Z_DIM = 100\n",
        "NUM_EPOCHS = 100\n",
        "FEATURES_CRITIC = 16\n",
        "FEATURES_GEN = 16\n",
        "CRITIC_ITERATIONS = 5\n",
        "LAMBDA_GP = 10\n",
        "train_loader = pneu_train_loader\n",
        "\n",
        "gen = Generator(Z_DIM, CHANNELS_IMG, FEATURES_GEN, NUM_CLASSES, IMG_SIZE, GEN_EMBEDDING).to(device)\n",
        "critic = Discriminator(CHANNELS_IMG, FEATURES_CRITIC, NUM_CLASSES, IMG_SIZE).to(device)\n",
        "initialize_weights(gen)\n",
        "initialize_weights(critic)\n",
        "\n",
        "# initializate optimizer\n",
        "opt_gen = optim.Adam(gen.parameters(), lr=LEARNING_RATE, betas=(0.0, 0.9))\n",
        "opt_critic = optim.Adam(critic.parameters(), lr=LEARNING_RATE, betas=(0.0, 0.9))\n",
        "\n",
        "step = 0\n",
        "\n",
        "gen.train()\n",
        "critic.train()\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    # Target labels not needed! <3 unsupervised\n",
        "    for batch_idx, (real, labels) in enumerate(tqdm(train_loader)):\n",
        "        real = real.to(device)\n",
        "        cur_batch_size = real.shape[0]\n",
        "        labels = labels.to(device)\n",
        "        # Train Critic: max E[critic(real)] - E[critic(fake)]\n",
        "        # equivalent to minimizing the negative of that\n",
        "        for _ in range(CRITIC_ITERATIONS):\n",
        "            noise = torch.randn(cur_batch_size, Z_DIM, 1, 1).to(device)\n",
        "\n",
        "            fake = gen(noise, labels)\n",
        "            # print(f\"noise shape = {real.shape}, labels shape = {labels.shape}\")\n",
        "            critic_real = critic(real, labels).reshape(-1)\n",
        "            critic_fake = critic(fake, labels).reshape(-1)\n",
        "            gp = gradient_penalty(critic, real, fake, labels, device=device)\n",
        "            loss_critic = (\n",
        "                -(torch.mean(critic_real) - torch.mean(critic_fake)) + LAMBDA_GP * gp\n",
        "            )\n",
        "            critic.zero_grad()\n",
        "            loss_critic.backward(retain_graph=True)\n",
        "            opt_critic.step()\n",
        "\n",
        "        # Train Generator: max E[critic(gen_fake)] <-> min -E[critic(gen_fake)]\n",
        "        gen_fake = critic(fake, labels).reshape(-1)\n",
        "        loss_gen = -torch.mean(gen_fake)\n",
        "        gen.zero_grad()\n",
        "        loss_gen.backward()\n",
        "        opt_gen.step()\n",
        "\n",
        "        # Print losses occasionally and print to tensorboard\n",
        "        if batch_idx % 10 == 0 and batch_idx > 0:\n",
        "            print(\n",
        "                f\"Epoch [{epoch}/{NUM_EPOCHS}] Batch {batch_idx}/{len(train_loader)} \\\n",
        "                  Loss D: {loss_critic:.4f}, loss G: {loss_gen:.4f}\"\n",
        "            )\n",
        "\n",
        "            with torch.no_grad():\n",
        "                fake = gen(noise, labels)\n",
        "\n",
        "                img_grid_real = torchvision.utils.make_grid(real[0], normalize=True)\n",
        "                img_grid_fake = torchvision.utils.make_grid(fake[0], normalize=True)\n",
        "\n",
        "                GOOGLE_DRIVE_PATH_fake = '/content/drive/My Drive/DL_FINAL/wgan/pneumnist_fake/'\n",
        "                os.makedirs(GOOGLE_DRIVE_PATH_fake, exist_ok=True)\n",
        "                GOOGLE_DRIVE_PATH_real = '/content/drive/My Drive/DL_FINAL/wgan/pneumnist_real/'\n",
        "                os.makedirs(GOOGLE_DRIVE_PATH_real,exist_ok=True)\n",
        "                torchvision.utils.save_image(img_grid_real, os.path.join(GOOGLE_DRIVE_PATH_real,f\"wgan_epoch_{epoch+1}.jpg\"))\n",
        "\n",
        "                torchvision.utils.save_image(img_grid_fake, os.path.join(GOOGLE_DRIVE_PATH_fake,f\"wgan_epoch_{epoch+1}.jpg\"))\n",
        "\n",
        "            step += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOQI6bEz6j7Q"
      },
      "source": [
        "### breastmnist_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qzxQOF1E_4n5",
        "outputId": "89f14f1e-fa8d-4e44-aa1e-3262a7a205e4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from tqdm import tqdm\n",
        "# Hyperparameters etc.\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "LEARNING_RATE = 5e-4\n",
        "BATCH_SIZE = 64\n",
        "IMG_SIZE = 28\n",
        "CHANNELS_IMG = 1\n",
        "NUM_CLASSES = 2\n",
        "GEN_EMBEDDING = 100\n",
        "Z_DIM = 100\n",
        "NUM_EPOCHS = 100\n",
        "FEATURES_CRITIC = 64\n",
        "FEATURES_GEN = 64\n",
        "CRITIC_ITERATIONS = 5\n",
        "LAMBDA_GP = 10\n",
        "train_loader = breast_train_loader\n",
        "\n",
        "gen = Generator(Z_DIM, CHANNELS_IMG, FEATURES_GEN, NUM_CLASSES, IMG_SIZE, GEN_EMBEDDING).to(device)\n",
        "critic = Discriminator(CHANNELS_IMG, FEATURES_CRITIC, NUM_CLASSES, IMG_SIZE).to(device)\n",
        "initialize_weights(gen)\n",
        "initialize_weights(critic)\n",
        "\n",
        "# initializate optimizer\n",
        "opt_gen = optim.Adam(gen.parameters(), lr=LEARNING_RATE, betas=(0.0, 0.9))\n",
        "opt_critic = optim.Adam(critic.parameters(), lr=LEARNING_RATE, betas=(0.0, 0.9))\n",
        "\n",
        "step = 0\n",
        "\n",
        "gen.train()\n",
        "critic.train()\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "\n",
        "    for batch_idx, (real, labels) in enumerate(tqdm(train_loader)):\n",
        "        real = real.to(device)\n",
        "        cur_batch_size = real.shape[0]\n",
        "        labels = labels.to(device)\n",
        "        # Train Critic: max E[critic(real)] - E[critic(fake)]\n",
        "        # equivalent to minimizing the negative of that\n",
        "        for _ in range(CRITIC_ITERATIONS):\n",
        "            noise = torch.randn(cur_batch_size, Z_DIM, 1, 1).to(device)\n",
        "\n",
        "            fake = gen(noise, labels)\n",
        "            # print(f\"noise shape = {real.shape}, labels shape = {labels.shape}\")\n",
        "            critic_real = critic(real, labels).reshape(-1)\n",
        "            critic_fake = critic(fake, labels).reshape(-1)\n",
        "            gp = gradient_penalty(critic, real, fake, labels, device=device)\n",
        "            loss_critic = (\n",
        "                -(torch.mean(critic_real) - torch.mean(critic_fake)) + LAMBDA_GP * gp\n",
        "            )\n",
        "            critic.zero_grad()\n",
        "            loss_critic.backward(retain_graph=True)\n",
        "            opt_critic.step()\n",
        "\n",
        "        # Train Generator: max E[critic(gen_fake)] <-> min -E[critic(gen_fake)]\n",
        "        gen_fake = critic(fake, labels).reshape(-1)\n",
        "        loss_gen = -torch.mean(gen_fake)\n",
        "        gen.zero_grad()\n",
        "        loss_gen.backward()\n",
        "        opt_gen.step()\n",
        "\n",
        "        # Print losses occasionally and print to tensorboard\n",
        "        if batch_idx % 10 == 0 and batch_idx > 0:\n",
        "            print(\n",
        "                f\"Epoch [{epoch}/{NUM_EPOCHS}] Batch {batch_idx}/{len(train_loader)} \\\n",
        "                  Loss D: {loss_critic:.4f}, loss G: {loss_gen:.4f}\"\n",
        "            )\n",
        "\n",
        "        with torch.no_grad():\n",
        "            fake = gen(noise, labels)\n",
        "\n",
        "            img_grid_real = torchvision.utils.make_grid(real[0], normalize=True)\n",
        "            img_grid_fake = torchvision.utils.make_grid(fake[0], normalize=True)\n",
        "            GOOGLE_DRIVE_PATH_fake = '/content/drive/My Drive/DL_FINAL/wgan/breast_fake/'\n",
        "            os.makedirs(GOOGLE_DRIVE_PATH_fake, exist_ok=True)\n",
        "            GOOGLE_DRIVE_PATH_real = '/content/drive/My Drive/DL_FINAL/wgan/breast_real/'\n",
        "            os.makedirs(GOOGLE_DRIVE_PATH_real,exist_ok=True)\n",
        "            torchvision.utils.save_image(img_grid_real, os.path.join(GOOGLE_DRIVE_PATH_real,f\"wgan_epoch_{epoch+1}.jpg\"))\n",
        "\n",
        "            torchvision.utils.save_image(img_grid_fake, os.path.join(GOOGLE_DRIVE_PATH_fake,f\"wgan_epoch_{epoch+1}.jpg\"))\n",
        "\n",
        "            # step += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ieWrk7jRRT0"
      },
      "source": [
        "### bloodmnist_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from tqdm import tqdm\n",
        "# Hyperparameters etc.\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "LEARNING_RATE = 1e-4 # todo\n",
        "BATCH_SIZE = 64 # todo\n",
        "IMG_SIZE = 28\n",
        "CHANNELS_IMG = 3\n",
        "NUM_CLASSES = 8\n",
        "GEN_EMBEDDING = 100\n",
        "Z_DIM = 100\n",
        "NUM_EPOCHS = 100\n",
        "FEATURES_CRITIC = 128 # 16, 64, 128\n",
        "FEATURES_GEN = 128 # 16, 64, 128\n",
        "CRITIC_ITERATIONS = 5\n",
        "LAMBDA_GP = 10\n",
        "train_loader = bloodmnist_train_loader\n",
        "gen = Generator(Z_DIM, CHANNELS_IMG, FEATURES_GEN, NUM_CLASSES, IMG_SIZE, GEN_EMBEDDING).to(device)\n",
        "critic = Discriminator(CHANNELS_IMG, FEATURES_CRITIC, NUM_CLASSES, IMG_SIZE).to(device)\n",
        "initialize_weights(gen)\n",
        "initialize_weights(critic)\n",
        "\n",
        "opt_gen = optim.Adam(gen.parameters(), lr=LEARNING_RATE, betas=(0.0, 0.9))\n",
        "opt_critic = optim.Adam(critic.parameters(), lr=LEARNING_RATE, betas=(0.0, 0.9))\n",
        "\n",
        "step = 0\n",
        "\n",
        "gen.train()\n",
        "critic.train()\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    # Target labels not needed! <3 unsupervised\n",
        "    for batch_idx, (real, labels) in enumerate(tqdm(train_loader)):\n",
        "        real = real.to(device)\n",
        "        cur_batch_size = real.shape[0]\n",
        "        labels = labels.to(device)\n",
        "        for _ in range(CRITIC_ITERATIONS):\n",
        "            noise = torch.randn(cur_batch_size, Z_DIM, 1, 1).to(device)\n",
        "\n",
        "            fake = gen(noise, labels)\n",
        "            # print(f\"noise shape = {real.shape}, labels shape = {labels.shape}\")\n",
        "            critic_real = critic(real, labels).reshape(-1)\n",
        "            critic_fake = critic(fake, labels).reshape(-1)\n",
        "            gp = gradient_penalty(critic, real, fake, labels, device=device)\n",
        "            loss_critic = (\n",
        "                -(torch.mean(critic_real) - torch.mean(critic_fake)) + LAMBDA_GP * gp\n",
        "            )\n",
        "            critic.zero_grad()\n",
        "            loss_critic.backward(retain_graph=True)\n",
        "            opt_critic.step()\n",
        "\n",
        "        # Train Generator: max E[critic(gen_fake)] <-> min -E[critic(gen_fake)]\n",
        "        gen_fake = critic(fake, labels).reshape(-1)\n",
        "        loss_gen = -torch.mean(gen_fake)\n",
        "        gen.zero_grad()\n",
        "        loss_gen.backward()\n",
        "        opt_gen.step()\n",
        "\n",
        "        # Print losses occasionally and print to tensorboard\n",
        "        if batch_idx % 10 == 0 and batch_idx > 0:\n",
        "            print(\n",
        "                f\"Epoch [{epoch}/{NUM_EPOCHS}] Batch {batch_idx}/{len(train_loader)} \\\n",
        "                  Loss D: {loss_critic:.4f}, loss G: {loss_gen:.4f}\"\n",
        "            )\n",
        "\n",
        "        with torch.no_grad():\n",
        "            fake = gen(noise, labels)\n",
        "\n",
        "            img_grid_real = torchvision.utils.make_grid(real[0], normalize=True)\n",
        "            img_grid_fake = torchvision.utils.make_grid(fake[0], normalize=True)\n",
        "            GOOGLE_DRIVE_PATH_fake = '/content/drive/My Drive/DL_FINAL/wgan/blood_fake/'\n",
        "            os.makedirs(GOOGLE_DRIVE_PATH_fake, exist_ok=True)\n",
        "            GOOGLE_DRIVE_PATH_real = '/content/drive/My Drive/DL_FINAL/wgan/blood_real/'\n",
        "            os.makedirs(GOOGLE_DRIVE_PATH_real,exist_ok=True)\n",
        "            torchvision.utils.save_image(img_grid_real, os.path.join(GOOGLE_DRIVE_PATH_real,f\"wgan_epoch_{epoch+1}.jpg\"))\n",
        "\n",
        "            torchvision.utils.save_image(img_grid_fake, os.path.join(GOOGLE_DRIVE_PATH_fake,f\"wgan_epoch_{epoch+1}.jpg\"))\n",
        "\n",
        "            # step += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Reference:\n",
        "https://github.com/aladdinpersson/Machine-Learning-Collection/tree/master/ML/Pytorch/GANs"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
