# -*- coding: utf-8 -*-
"""CS671.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1E25dolKlFb9z2whnKoxSjhwc0R1EXXxk

## Exploratory Data Analysis
"""

# Commented out IPython magic to ensure Python compatibility.
# %load_ext autoreload
# %autoreload 2
from google.colab import drive
import os
import pandas as pd
import ast
import numpy as np
import sys
import pandas as pd

import matplotlib.pyplot as plt
import seaborn as sns
drive.mount("/content/drive")

GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = "CS671FINAL"
GOOGLE_DRIVE_PATH = os.path.join("drive", "My Drive", GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)
print(os.listdir(GOOGLE_DRIVE_PATH))
sys.path.append(GOOGLE_DRIVE_PATH)

"""### Raw Data Analysis"""

train_file_path = os.path.join(GOOGLE_DRIVE_PATH, 'train.csv')
train = pd.read_csv(train_file_path) # don't read the first unnamed column

print(f"train shape = {train.shape}")
print(f"There are {len(train.columns)} columns in training dataset. The column names = {train.columns}") # 47

numeric_cols = train.select_dtypes(include=[np.number])
data_types = train.dtypes
object_cols = train.select_dtypes(include=['object'])

print(f"There are {len(numeric_cols.columns)} numerical columns. They are {numeric_cols.columns}") # 28
print(f"There are {len(object_cols.columns)} object columns. They are {object_cols.columns}") # 28

# Assuming 'df' is your DataFrame
na_rows_count = train.isna().any(axis=1).sum()
print(f"There are {na_rows_count} rows in training dataset having at least one NA value.")

test_file_path = os.path.join(GOOGLE_DRIVE_PATH, 'test.csv')
test = pd.read_csv(test_file_path) # don't read the first unnamed column

print(f"Test Shape = {test.shape}")
print(f"There are {len(test.columns)} columns in test dataset. The column names = {train.columns}") # 47

"""###### Pick Selected Features from train_sentiment.csv saved at train_selected.csv"""

feature_columns = ['id','neighbourhood_group_cleansed', 'latitude', 'longitude', 'host_id', 'host_is_superhost', 'host_listings_count', 'host_total_listings_count',
                   'host_has_profile_pic', 'host_identity_verified', 'accommodates', 'bathrooms_text', 'beds', 'amenities', 'minimum_nights', 'maximum_nights', 'minimum_minimum_nights',
                   'maximum_minimum_nights', 'minimum_maximum_nights', 'maximum_maximum_nights', 'minimum_nights_avg_ntm', 'maximum_nights_avg_ntm', 'has_availability', 'availability_30', 'availability_60','availability_90', 'availability_365',
                   'number_of_reviews', 'number_of_reviews_ltm', 'number_of_reviews_l30d', 'calculated_host_listings_count', 'calculated_host_listings_count_entire_homes',
           'calculated_host_listings_count_private_rooms', 'calculated_host_listings_count_shared_rooms', 'Sentiment', 'instant_bookable', 'room_type', 'property_type', 'host_verifications', 'host_since','price', 'description']
feature_columns_test =  ['id','neighbourhood_group_cleansed', 'latitude', 'longitude', 'host_id', 'host_is_superhost', 'host_listings_count', 'host_total_listings_count',
                   'host_has_profile_pic', 'host_identity_verified', 'accommodates', 'bathrooms_text', 'beds', 'amenities', 'minimum_nights', 'maximum_nights', 'minimum_minimum_nights',
                   'maximum_minimum_nights', 'minimum_maximum_nights', 'maximum_maximum_nights', 'minimum_nights_avg_ntm', 'maximum_nights_avg_ntm', 'has_availability', 'availability_30', 'availability_60','availability_90', 'availability_365',
                   'number_of_reviews', 'number_of_reviews_ltm', 'number_of_reviews_l30d', 'calculated_host_listings_count', 'calculated_host_listings_count_entire_homes',
           'calculated_host_listings_count_private_rooms', 'calculated_host_listings_count_shared_rooms', 'Sentiment', 'instant_bookable', 'room_type', 'property_type', 'host_verifications','host_since', 'description']

print(f"len of feature columns of training = {len(feature_columns)}")

train_file_path = os.path.join(GOOGLE_DRIVE_PATH, 'train_sentiment.csv')
train = pd.read_csv(train_file_path) # don't read the first unnamed column
na_rows_count = train.isna().any(axis=1).sum()
print(f"There are {na_rows_count} rows in training dataset having at least one NA value.")
selected_columns =train[feature_columns]
selected_columns.to_csv('/content/drive/My Drive/CS671FINAL/train_selected_des.csv', index=False) # keep id column


test_file_path = os.path.join("/content/drive/My Drive/CS671FINAL", 'test_sentiment.csv')
test = pd.read_csv(test_file_path) # don't read the first unnamed column
selected_columns =test[feature_columns_test]
na_rows_count = train.isna().any(axis=1).sum()
print(f"There are {na_rows_count} rows in training dataset having at least one NA value.")
selected_columns.to_csv('/content/drive/My Drive/CS671FINAL/test_selected_des.csv', index=False) # keep id column

"""### Dataset Processing"""

# import pandas as pd

# # Replace 'path_to_csv' with your actual file path
train_file_path = os.path.join(GOOGLE_DRIVE_PATH, 'train_selected.csv')
test_file_path = os.path.join(GOOGLE_DRIVE_PATH, 'test_selected.csv')

# # Load the CSV file into a DataFrame
train = pd.read_csv('/content/drive/My Drive/CS671FINAL/train_selected_des.csv') # don't read the first unnamed column
test = pd.read_csv('/content/drive/My Drive/CS671FINAL/test_selected_des.csv')
print(f"shape of train file: {train.shape}")

print(f"shape of test file: {test.shape}") # for prediction
na_rows_count = train.isna().any(axis=1).sum()
print(f"There are {na_rows_count} rows in training dataset having at least one NA value.")

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

df = train
df_test = test

feature_columns = ['neighbourhood_group_cleansed', 'latitude', 'longitude', 'host_id', 'host_is_superhost', 'host_listings_count', 'host_total_listings_count',
                   'host_has_profile_pic', 'host_identity_verified', 'accommodates', 'bathrooms_text', 'beds', 'amenities', 'minimum_nights', 'maximum_nights', 'minimum_minimum_nights',
                   'maximum_minimum_nights', 'minimum_maximum_nights', 'maximum_maximum_nights', 'minimum_nights_avg_ntm', 'maximum_nights_avg_ntm', 'has_availability', 'availability_30', 'availability_60','availability_90', 'availability_365',
                   'number_of_reviews', 'number_of_reviews_ltm', 'number_of_reviews_l30d', 'calculated_host_listings_count', 'calculated_host_listings_count_entire_homes',
           'calculated_host_listings_count_private_rooms', 'calculated_host_listings_count_shared_rooms', 'Sentiment', 'instant_bookable', 'room_type', 'property_type', 'host_verifications', 'host_since', 'price', 'description']



tf_features = ['host_has_profile_pic', 'host_identity_verified', 'has_availability', 'instant_bookable', 'host_is_superhost']
category_features = ['neighbourhood_group_cleansed', 'room_type', 'host_verifications']
text_features = ['amenities', 'property_type', 'bathrooms_text', 'Sentiment','host_since', 'description']
target =['price']
test_features = [f for f in feature_columns if f not in target]
numerical_features = [f for f in feature_columns if f not in (tf_features + category_features + text_features+target)]
print(f"length of numerical featrues = {len(numerical_features)}")

"""#### Distribution of numerical value

"""

import pandas as pd
import matplotlib.pyplot as plt

# Assuming df is your DataFrame
# Replace these column names with the columns you want to plot
columns_to_plot = numerical_features

# Calculating means and standard deviations
means = [df[col].mean() for col in columns_to_plot]
stds = [df[col].std() for col in columns_to_plot]

import matplotlib.pyplot as plt

# Assuming 'means' and 'stds' are your calculated mean and standard deviation lists
# and 'columns_to_plot' is your list of column names

plt.figure(figsize=(20, 10))  # Adjust the figure size as necessary
bar_positions = range(len(columns_to_plot))
plt.bar(bar_positions, means, yerr=stds, color='skyblue', capsize=5, error_kw={'elinewidth':2, 'ecolor':'black'})

# Rotate x-axis labels for better readability
plt.xticks(bar_positions, columns_to_plot, rotation=45, ha='right', fontsize=12)

# If the range of values is very large, using a logarithmic scale can be helpful
plt.yscale('log')

# Adding labels, title, and grid
plt.xlabel('Columns', fontsize=14)
plt.ylabel('Mean Values', fontsize=14)
plt.title('Mean and Standard Deviation of Numerical Columns', fontsize=16)
plt.grid(True)

# Show the plot
plt.tight_layout()  # Adjust the padding between and around subplots
plt.show()
plt.savefig("distribution")

"""#### Plot the Distribution of Target Label

"""

import matplotlib.pyplot as plt
import pandas as pd

# Assuming df is your DataFrame and it has a column named 'price'
# df = pd.DataFrame(...)  # your DataFrame loading/creation here

# Plotting
plt.figure(figsize=(10, 6))  # Adjust the size of the figure as needed
plt.hist(df['price'], bins=20, edgecolor='black')  # You can adjust the number of bins
plt.title('Histogram of Prices')
plt.xlabel('Price')
plt.ylabel('Frequency')
plt.grid(True)  # Adds a grid for better readability
plt.show()

"""#### Categorical Variables:"""

"""
true or false --> 0 or 1
"""
# df[tf_features] = df[tf_features].astype(int)
import pandas as pd
tf_features = ['host_has_profile_pic', 'host_identity_verified', 'has_availability', 'instant_bookable', 'host_is_superhost']

# Define a mapping dictionary
mapping = {'t': 1, 'f': 0}

# Apply the mapping to the desired columns
df['host_has_profile_pic'] = df['host_has_profile_pic'].astype(str).map(mapping)
df['host_identity_verified'] = df['host_identity_verified'].astype(str).map(mapping)
df['has_availability'] = df['has_availability'].astype(str).map(mapping)
df['instant_bookable'] = df['instant_bookable'].astype(str).map(mapping)
df['host_is_superhost'] = df['host_is_superhost'].astype(str).map(mapping)

df_test['host_has_profile_pic'] = df_test['host_has_profile_pic'].astype(str).map(mapping)
df_test['host_identity_verified'] = df_test['host_identity_verified'].astype(str).map(mapping)
df_test['has_availability'] = df_test['has_availability'].astype(str).map(mapping)
df_test['instant_bookable'] = df_test['instant_bookable'].astype(str).map(mapping)
df_test['host_is_superhost'] = df_test['host_is_superhost'].astype(str).map(mapping)

# The DataFrame after conversion
print(df.columns)
print(len(df.columns))

"""
multilabel --> one hot encoding; includ newly added
"""
df = pd.get_dummies(df, columns=['room_type'])
df = pd.get_dummies(df, columns=['Sentiment'])
# neighbourhood_group_cleansed
df = pd.get_dummies(df, columns=['neighbourhood_group_cleansed'])

df_test = pd.get_dummies(df_test, columns=['room_type'])
df_test = pd.get_dummies(df_test, columns=['Sentiment'])
# neighbourhood_group_cleansed
df_test = pd.get_dummies(df_test, columns=['neighbourhood_group_cleansed'])

# Drop Sentiment = -1: because I assign -1 when no sentiment can be predicted
df.drop('Sentiment_-1', axis=1, inplace=True)
df_test.drop('Sentiment_-1', axis=1, inplace=True)
print(df.columns)
print(len(df.columns))

"""
Multi-label: host_verification
"""
df['host_verifications'] = df['host_verifications'].apply(ast.literal_eval)

# Identify all unique labels
all_labels = sorted(set(label for row in df['host_verifications'] for label in row))

# Create a mapping of labels to indices
label_to_index = {label: idx for idx, label in enumerate(all_labels)}

# One-hot encode each row and append the result to the DataFrame
for label in all_labels:
    # Create a new column for each label, setting to 1 where the label is present in the row, else 0
    df[f'host_verification_{label}'] = df['host_verifications'].apply(lambda x: int(label in x))

# Now df has the original 'host_verifications' column and additional columns for the one-hot encoded features
# print(df.columns)
df.drop('host_verifications', axis=1, inplace=True)


df_test['host_verifications'] = df_test['host_verifications'].apply(ast.literal_eval)

# Identify all unique labels
all_labels = sorted(set(label for row in df_test['host_verifications'] for label in row))

# Create a mapping of labels to indices
label_to_index = {label: idx for idx, label in enumerate(all_labels)}

# One-hot encode each row and append the result to the DataFrame
for label in all_labels:
    # Create a new column for each label, setting to 1 where the label is present in the row, else 0
    df_test[f'host_verification_{label}'] = df_test['host_verifications'].apply(lambda x: int(label in x))

# Now df has the original 'host_verifications' column and additional columns for the one-hot encoded features
print(df_test.columns)
df_test.drop('host_verifications', axis=1, inplace=True)

"""###### Unique class in categorical variable"""

"""
Decide to treat as text or one hot variables
"""
# Assuming df is your DataFrame
combined_list = []
# deal with property type:
unique_values_property = df['property_type'].unique()
# deal with room type:
unique_values_room = df['room_type'].unique()

unique_values_neigh_group = df['neighbourhood_group_cleansed'].unique()
print(f"number of unique group = {len(unique_values_neigh_group)}")
print(f"number of unique property = {len(unique_values_property)}, number of unique room = {len(unique_values_room)}")

"""#### Text"""

"""
Time: host since
"""
df['host_since'] = pd.to_datetime(df['host_since'])

# Calculate the difference in days between today and the date column
df['days_difference'] = (pd.Timestamp.today() - df['host_since']).dt.days

df.drop('host_since', axis=1, inplace=True)
print(df['days_difference'].head())
print(df.columns)

df_test['host_since'] = pd.to_datetime(df_test['host_since'])

# Calculate the difference in days between today and the date column
df_test['days_difference'] = (pd.Timestamp.today() - df_test['host_since']).dt.days

df_test.drop('host_since', axis=1, inplace=True)
print(df_test['days_difference'].head())
print(df_test.columns)

"""
text: amenities: transform, no need for noramlize, default is l2 norm
"""

from sklearn.feature_extraction.text import TfidfVectorizer


import pandas as pd

# check NA type for amenities
missing_values = df['amenities'].isna().any()
print(f"missing values = {missing_values}")

vectorizer = TfidfVectorizer(max_features = 50, stop_words='english')
text_features_df = vectorizer.fit_transform(df['amenities'])


text_features_df = pd.DataFrame(text_features_df.toarray(),columns=vectorizer.get_feature_names_out())
df= pd.concat([df.drop('amenities', axis=1), text_features_df], axis=1) # expand more columns

print(df.columns)


# Do not refit on the test data
text_features_test = vectorizer.transform(df_test['amenities'])

text_features_df_test = pd.DataFrame(text_features_test.toarray(), columns=vectorizer.get_feature_names_out()) # transfer back to panda dataframe
df_test= pd.concat([df_test.drop('amenities', axis=1), text_features_df_test], axis=1) # expand more columns
print(df_test.columns)

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import StandardScaler
import pandas as pd

# check NA value for property_type
missing_values = df['property_type'].isna().any()
print(f"missing values = {missing_values}")

vectorizer = TfidfVectorizer(max_features=50, stop_words='english')
text_features = vectorizer.fit_transform(df['property_type'])

scaled_text_features_df = pd.DataFrame(text_features.toarray(), columns=vectorizer.get_feature_names_out())
df = pd.concat([df.drop('property_type', axis=1), scaled_text_features_df], axis=1)


text_features_test = vectorizer.transform(df_test['property_type'])

text_features_df_test = pd.DataFrame(text_features_test.toarray(), columns=vectorizer.get_feature_names_out())
df_test = pd.concat([df_test.drop('property_type', axis=1), text_features_df_test], axis=1)

# import pandas as pd
# import numpy as np
# import torch
# import torchtext
glove = torchtext.vocab.GloVe(name='6B', dim=100)  # For example, using 100-dimensional embeddings

def get_text_embedding(text, glove_vectors):
    text = str(text)
    tokens = text.lower().split()
    embeddings = [glove_vectors[word] for word in tokens if word in glove_vectors.stoi]
    if embeddings:
        embeddings = torch.stack(embeddings)
        avg_embedding = embeddings.mean(dim=0).numpy()
    else:
        avg_embedding = np.zeros(glove_vectors.dim)  # Return a zero vector if text is empty or all words are out-of-vocabulary

    return avg_embedding

# Applying the function and expanding the results into separate columns
for i in range(glove.dim):
    df[f'text_emb_{i+1}'] = df['description'].apply(lambda x: get_text_embedding(x, glove)[i])

# import pandas as pd
# import numpy as np
# import torch
# import torchtext
# glove = torchtext.vocab.GloVe(name='6B', dim=100)  # For example, using 100-dimensional embeddings

def get_text_embedding(text, glove_vectors):
    text = str(text)
    tokens = text.lower().split()
    embeddings = [glove_vectors[word] for word in tokens if word in glove_vectors.stoi]
    if embeddings:
        embeddings = torch.stack(embeddings)
        avg_embedding = embeddings.mean(dim=0).numpy()
    else:
        avg_embedding = np.zeros(glove_vectors.dim)  # Return a zero vector if text is empty or all words are out-of-vocabulary

    return avg_embedding

# Applying the function and expanding the results into separate columns
for i in range(glove.dim):
    df_test[f'text_emb_{i+1}'] = df_test['description'].apply(lambda x: get_text_embedding(x, glove)[i])

# # tokenize the text of bath
"""
text: bath
"""
# ! pip install nltk # for tokenize
import pandas as pd
import nltk

# nltk.download('punkt')
# Ensure you have the necessary tokenizer downloaded

def tokenize_text(text):
    # Check if the text is None or not
    if text is None:
        print("Text is None, returning 'N/A'.")
        return None
    try:
        # Now we can be sure that text is not None and try to tokenize it
        # print(f"text = {text.split()}")
        return text.split()
    except Exception as e:
        print(f"Text cannot be tokenized: {text} - Error: {e}")
        return None


# na_rows_count = df['bathrooms_text'].isna().any()
# print(f"There are {na_rows_count} rows in training dataset having at least one NA value.")

# Drop the rows where df['bathrooms_text'] = None. Only one row

# Apply the function to each item in the column, assuming 'df' is your DataFrame
df['bathrooms_text'] =  df['bathrooms_text'].astype(str) # important, expect a string
# na_rows_count = df['bathrooms_text'].isna().any()
# print(f"There are {na_rows_count} rows in training dataset having at least one NA value.")

df['bathrooms_text'] = df['bathrooms_text'].apply(tokenize_text)
# df_test['bathrooms_text'] =  df_test['bathrooms_text'].astype(str) # important, expect a string
# df_test['bathrooms_text'] = df_test['bathrooms_text'].apply(tokenize_text)

def conversion(text_str):
  need_division = False

  if "shared" in text_str:
    need_division = True

  for i in range(len(text_str)):
    try:
      float(text_str[i])
      is_number = True
    except ValueError:
      is_number = False

    if is_number == True: # if it is digit
      # print(f"text str = {text_str[i]}")
      if need_division:
        res = float(text_str[i])/2
        # print(f"res /2 = {res}")
        return res # split if shared
      else:
        res = float(text_str[i])
        # print(f"res = {res}")
        return res # don't split
    else:
      return 0 # no bath room
df['bathrooms_text'] = df['bathrooms_text'].apply(conversion)

df_test['bathrooms_text'] = df_test['bathrooms_text'].apply(conversion)

print(f"len = {len(df.columns)}")
print(f"len = {len(df_test.columns)}")

"""
noralize on numerical data; include newly added
"""

from sklearn.preprocessing import StandardScaler
import pandas as pd


# Assuming df is your DataFrame and numerical_features is your list of numerical columns
scaler = StandardScaler()
# Standard scaler is sensitive to NA vlaues, drop those rows with NA values

na_rows_count = df.isna().any(axis=1).sum()
print(f"There are {na_rows_count} rows in training dataset having at least one NA value.")


numerical_features = numerical_features + ['days_difference'] + ['bathrooms_text']
print(df.columns)

df= df.dropna(subset=numerical_features)

print(f"After dealing with NA values, my dataframe has shape = {df.shape}") # probably need those processed

# Scale the numerical features and replace them in the DataFrame
df[numerical_features] = scaler.fit_transform(df[numerical_features])

# fit the mean and value of the train set to the test set
df_test[numerical_features] = scaler.transform(df_test[numerical_features])

"""Saved processed csv as train_processed.csv"""

df.to_csv(os.path.join(GOOGLE_DRIVE_PATH, 'train_processed_des_bert.csv'), index=False)
df_test.to_csv(os.path.join(GOOGLE_DRIVE_PATH, 'test_processed_des_bert.csv'), index=False)

"""###### Load Pretained Model and Perform Sentimental Analysis on it \\
Distilbert-base-uncased-emotion is a model fine-tuned for detecting emotions in texts, including sadness, joy, love, anger, fear and surprise\\
Saved at train_sentiment.csv

"""

# !pip install -q transformers
# from transformers import pipeline

# # Initialize sentiment analysis pipeline
# sentiment_pipeline = pipeline("text-classification",model='bhadresh-savani/distilbert-base-uncased-emotion', return_all_scores=True)
# sentiment_list = ['sadness', 'joy', 'love', 'anger', 'fear', 'surprise']
# # Function to get sentiment
# def get_sentiment(text):
#   # print(f"type of text = {type(text)}")
#   if type(text) != str:
#     print(f"error input, should be all zeros")
#     return -1
#   try:
#     prediction = sentiment_pipeline(text) # should be a dictionary
#     emotion_scores = {}
#     # print(f"column")
#     for i in range(5):
#       emotion_scores[prediction[0][i]['label']] = prediction[0][i]['score']
#       max_emotion = max(emotion_scores, key=emotion_scores.get) # get the emotion with the highest score, emotion is a nnumber

#     # one_hot_vector = [1 if emotion == max_emotion else 0 for emotion in emotion_scores]
#     # index = sentiment_list.index(str(max_emotion))
#     # print(f"max_emotion = {index}") # one prediction should have one max emotion one --> checked one row one prediction
#     return str(max_emotion)
#   except:
#     print(f"error input, should be all zeros")
#     return -1
#   # prediction = sentiment_pipeline(text) # should be a dictionary
#   # res = sentiment_pipeline(text) # should be a dictionary
#   # print(f"res = {res}")


# # Apply sentiment analysis to the description column
# test['Sentiment'] = test['description'].apply(get_sentiment)

# # Save the dataframe back to CSV
# test.to_csv('test_sentiment.csv', index=False) #

# # saved at google colab

"""### Data Split

#### Load Processed Data
"""

train_file_path = os.path.join(GOOGLE_DRIVE_PATH, 'train_processed_des_glove.csv')
train = pd.read_csv(train_file_path) # don't read the first unnamed column
print(train.shape)

test_file_path = os.path.join(GOOGLE_DRIVE_PATH, 'test_processed_des_glove.csv')
test = pd.read_csv(test_file_path) # don't read the first unnamed column

print(test.shape)

"""###### Distribution of Important Variable"""

import matplotlib.pyplot as plt
import seaborn as sns

# List of columns you want to plot
columns_to_plot = ['accommodates', 'bathrooms_text', 'room_type_Entire home/apt', 'room' ]

# Number of rows/columns for the subplot grid
num_rows = 2
num_cols = 2  # Adjust as needed

# Set the style of seaborn for better aesthetics
sns.set(style='whitegrid')

# Create a figure with subplots
plt.figure(figsize=(12, 10))  # Adjust size as needed

for i, column in enumerate(columns_to_plot):
    plt.subplot(num_rows, num_cols, i + 1)
    sns.histplot(train[column], color='blue', label='Training Set', kde=True)
    sns.histplot(test[column], color='red', label='Test Set', kde=True)
    plt.title(f'Distribution of {column}')
    plt.xlabel(column)
    plt.ylabel('Frequency')
    plt.legend()

plt.tight_layout()
plt.show()

# train.drop('description', axis=1, inplace=True)


# Separate the target variable ('price')
y = train['price'].values

# Drop the 'price' column to create the features DataFrame and then get its values
X = train.drop('price', axis=1).values

true_test = test.values
print(f"true test shape = {test.shape}")

# Split the data into training and test sets (80% train, 20% test)
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
print(f"X_train.shape = {X_train.shape}")
print(f"X_test.shape = {X_test.shape}")

train_filtered = train.dropna()
# train_filtered.drop('description', axis=1, inplace=True)
y_filtered = train_filtered['price'].values

# Drop the 'price' column to create the features DataFrame and then get its values
X_filtered = train_filtered.drop('price', axis=1).values

# Perform PCA on filteed
from sklearn.decomposition import PCA

pca = PCA(n_components=50)
pca.fit(X_filtered)
X_filtered = pca.transform(X_filtered)
true_pca = pca.transform(true_test)
print(f"shape of output of PCA = {X_filtered.shape}")
print(f"shape of output of PCA = {true_pca.shape}")

# Split the data into training and test sets (80% train, 20% test)
X_train_filtered, X_test_filtered, y_train_filtered, y_test_filtered = train_test_split(X_filtered, y_filtered, test_size=0.2, random_state=42)
print(f"X_train.shape = {X_train_filtered.shape}")
print(f"X_test.shape = {X_test_filtered.shape}")

"""### Models

### Baseline
"""

from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report


# Assuming 'X' and 'y' are your features and labels
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


# Initialize the Logistic Regression model for multiclass
model = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000)

# Fit the model
model.fit(X_train_filtered, y_train_filtered) # sensitive to NAN

# Predict and evaluate
y_pred = model.predict(X_test_filtered)
print(classification_report(y_test_filtered, y_pred))
from sklearn.metrics import accuracy_score

# Assuming y_pred contains the predicted labels for the test set
test_accuracy = accuracy_score(y_test_filtered, y_pred)

# Print the test accuracy
print(f"Test Accuracy: {test_accuracy * 100:.2f}%")

"""### XGBoost
Define my problem: multiclass classification
"""

!pip install xgboost
import xgboost as xgb

# record time
import time
start_time = time.time()

import xgboost as xgb
from sklearn.metrics import accuracy_score

# 1. Create the XGBoost classifier object
xg_clf = xgb.XGBClassifier(
    n_estimators=1000,
    learning_rate=0.1,
    max_depth=20,
    min_child_weight=1,

    # other parameters
)
# 2. Fit the classifier to the training set
xg_clf.fit(X_train_filtered, y_train_filtered)

# 3. Predict on the test set
y_pred = xg_clf.predict(X_test_filtered)

# 4. Evaluate the classifier
accuracy = accuracy_score(y_test_filtered, y_test_filtered)
print(f"Accuracy: {accuracy}")

import time
start_time = time.time()
from sklearn.metrics import accuracy_score

# 1. Create the XGBoost classifier object
xg_clf = xgb.XGBClassifier(
    n_estimators=1000,
    learning_rate=0.1,
    max_depth=9,
    min_child_weight=1,
    colsample_bytree=0.6,
    subsample=0.8,
    gamma=0.8,

    # other parameters
)
# 2. Fit the classifier to the training set
xg_clf.fit(X_train, y_train)
training_time = time.time() - start_time
# 3. Predict on the test set
y_pred = xg_clf.predict(X_test)

# 4. Evaluate the classifier
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy}, Training for = {training_time}")

"""##### Hyperparameter Searching

eta: learning rate: 0.01-0.2: choose a relative high lr



min_child_weight: tune with cv

max_depth 3-10

gamma: specifies the minimum loss reduction required to make a split, depending on loss function

lambda: L2 regularization

alpha: L1 regulariztiaon

colsample_bytree: denotes the fraction of observations to be random samples for each tree: 0.5-1
"""

import matplotlib.pyplot as plt
import numpy as np
from sklearn.model_selection import cross_val_score
# Assuming X_train, y_train are defined
# Define the parameter grid
param_grid = {
    'max_depth': [6, 9],
    'min_child_weight': [1, 3, 5]
}

# Initialize dictionary to store results
results = {}

# Loop through parameter combinations
for max_depth in param_grid['max_depth']:
    for child_weight in param_grid['min_child_weight']:
        # Create XGBoost model with current parameters
        xgb_model = xgb.XGBClassifier(max_depth=max_depth, min_child_weight=child_weight,
                                      learning_rate=0.1, n_estimators=1000)

        # Perform cross-validation and calculate mean accuracy
        scores = cross_val_score(xgb_model, X_train, y_train, cv=5, scoring='accuracy')
        mean_accuracy = np.mean(scores)
        print(f"max_depth = {max_depth}, min_child_weight = {child_weight}, mean_accuracy = {mean_accuracy}")
        # Store results
        results[(max_depth, child_weight)] = mean_accuracy

# Prepare data for plotting
max_depths, child_weights, accuracies = zip(*[(md, cw, acc) for (md, cw), acc in results.items()])

# Convert to NumPy arrays for ease of manipulation
max_depths = np.array(max_depths)
child_weights = np.array(child_weights)
accuracies = np.array(accuracies)

# Plotting
plt.figure(figsize=(10, 6))
for depth in param_grid['max_depth']:
    indices = max_depths == depth
    plt.plot(child_weights[indices], accuracies[indices], label=f'max_depth={depth}')

plt.xlabel('max_child_weight')
plt.ylabel('Mean Accuracy')
plt.title('Accuracy vs. Hyperparameters')
plt.legend()
plt.grid(True)
plt.show()

param_grid = {
  'gamma':[0.5,0.8,1]
}
for g in param_grid['gamma']:
    # Create XGBoost model with current parameters
    xgb_model = xgb.XGBClassifier(max_depth=9, min_child_weight=3,
                                  learning_rate=0.1, n_estimators=1000, gamma = g)

    # Perform cross-validation and calculate mean accuracy
    scores = cross_val_score(xgb_model, X_train, y_train, cv=5, scoring='accuracy')
    mean_accuracy = np.mean(scores)
    print(f"gamma = {g}, mean_accuracy = {mean_accuracy}")

from sklearn.model_selection import cross_val_score
param_grid = {
  'alpha':[0.5,0.8,1]
}
for l in param_grid['alpha']:
    # Create XGBoost model with current parameters
    xgb_model = xgb.XGBClassifier(max_depth=9, min_child_weight=5,learning_rate=0.1, alpha = l, n_estimators=1000, gamma = 0.5)

    # Perform cross-validation and calculate mean accuracy
    scores = cross_val_score(xgb_model, X_train, y_train, cv=5, scoring='accuracy')
    mean_accuracy = np.mean(scores)
    print(f"alpha = {l}, mean_accuracy = {mean_accuracy}")

"""##### Predict on test set"""

# feature importance
import matplotlib.pyplot as plt
import xgboost as xgb

# Assuming xg_clf is your trained XGBoost model
# Get feature importances
feature_importances = xg_clf.feature_importances_
train.drop(['price'], axis=1, inplace=True)
# Get feature names
feature_names = train.columns
# Create a series with feature importances and labels
importance_series = pd.Series(feature_importances, index=[feature_names])

# Sort the feature importances
importance_series_sorted = importance_series.sort_values(ascending=False).head(20)

# Plot
plt.figure(figsize=(10, 6))
importance_series_sorted.plot(kind='bar')
plt.title('Most important 20 features')
plt.xlabel('Features')
plt.ylabel('Importance')
plt.show()

test_pred = xg_clf.predict(true_test)
predictions_df = pd.DataFrame(test_pred, columns=['Price'])

# Create a new column for row ID, starting from 0
predictions_df['id'] = range(0, len(predictions_df))

# Reorder the columns to have 'ID' first and 'price' second
predictions_df = predictions_df[['id', 'price']]
print(f"submission = {predictions_df.shape}")
# Save to CSV with the new row ID as the first column
predictions_df.to_csv('/content/drive/My Drive/CS671FINAL/submission.csv', index=False)

"""### Neural Network"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import pandas as pd
import matplotlib.pyplot as plt
from torch.utils.data import Dataset
from torch.utils.data import DataLoader
import numpy as np

# NOT GOOD AT DEAL WITH NA VALUE, NEED TO DROP
# fill the NA value with mode value, we
df = train
na_columns = df.isna().any()

# Print out columns with NA values
for column, has_na in na_columns.items():
    if has_na:
        print(column)

"""#### Make custom dataset"""

class CustomDataset(Dataset):
    def __init__(self, features, labels):
        """
        Initialize the dataset with features and labels.
        :param features: A list or array of features (X_train).
        :param labels: A list or array of labels (y_train).
        """
        self.features = features
        self.labels = labels

    def __len__(self):
        """
        Return the total number of samples in the dataset.
        """
        return len(self.labels)

    def __getitem__(self, idx):
        """
        Generate one sample of data.
        :param idx: The index of the sample.
        """
        # Get the sample and its corresponding label
        sample = self.features[idx]
        label = self.labels[idx]

        # Convert to PyTorch tensors
        sample_tensor = torch.tensor(sample, dtype=torch.float32)
        label_tensor = torch.tensor(label, dtype=torch.float32)  # Use torch.long for classification

        return sample_tensor, label_tensor

"""#### Training"""

train_dataset = CustomDataset(X_train_filtered, y_train_filtered.ravel())
valid_dataset = CustomDataset(X_test_filtered, y_test_filtered.ravel())
print(f"train dataset shape = {len(train_dataset)}")
from torch.utils.data import DataLoader
# Parameters
batch_size = 512
# Create the DataLoaders
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)
for data, label in train_loader:
    print(f"data shape = {data.shape}")
    print(f"label shape = {label.shape}")
    break

# training args
num_epochs = 10

## need initialization
# treat description as an embedding layer as it has a brief summary of the facilities and might involve customer's atttitude over the Airbnb

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.fc1 = nn.Linear(144, 1024)
        self.fc2 = nn.Linear(1024, 512)
        self.fc3 = nn.Linear(512, 256)
        self.fc4 = nn.Linear(256, 128)
        self.fc5 = nn.Linear(128, 6)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = F.relu(self.fc3(x))
        x = F.relu(self.fc4(x))
        x = self.fc5(x)  # No softmax needed for CrossEntropyLoss
        return x

# Initialize the network
net = Net()

# If using CUDA (uncomment if you're using GPU)
net.to("cuda")

# Loss and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(net.parameters(), lr=0.001, weight_decay = 1e-5) # add L2 regualriztiaon

# Training loop
epochs = 100
for epoch in range(epochs):
    total_loss = 0
    for data, label in train_loader:
        # Flatten data and transfer to CUDA if necessary
        data = data.view(-1, 144)  # Reshape data
        label = label.long()
        # if using CUDA (uncomment if you're using GPU)
        data, label = data.to("cuda"), label.to("cuda")

        optimizer.zero_grad()
        outputs = net(data)
        loss = criterion(outputs, label)
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    average_loss = total_loss / len(train_loader)
    print(f'Epoch {epoch+1}, Average Training Loss: {average_loss}')

    # Validation loop
    net.eval()  # Set the model to evaluation mode
    with torch.no_grad():
        total_loss = 0
        total_correct = 0
        total_samples = 0
        for data, label in valid_loader:
            data = data.view(-1, 144)  # Reshape data
            # if using CUDA (uncomment if you're using GPU)
            data, label = data.to("cuda"), label.to("cuda")
            label = label.long()
            outputs = net(data)
            loss = criterion(outputs, label)
            total_loss += loss.item()

            _, predicted = torch.max(outputs.data, 1)
            total_correct += (predicted == label).sum().item()
            total_samples += label.size(0)

        average_loss = total_loss / len(valid_loader)
        accuracy = 100 * total_correct / total_samples
        print(f'Average Validation Loss: {average_loss}, Accuracy: {accuracy}%')

    net.train()  # Set the model back to training mode



"""### KNN"""

import torch

def euclidean_distance(tensor1, tensor2):
    return torch.sqrt(torch.sum((tensor1 - tensor2) ** 2, dim=1))

def knn(X_train, y_train, X_test, k=3):
    y_pred = torch.zeros(X_test.size(0), dtype=torch.long)
    for i, test_sample in enumerate(X_test):
        # Calculate distances from this test point to all training samples
        distances = euclidean_distance(test_sample, X_train)
        # Sort distances, and return the indices of k nearest neighbors
        k_neighbors = torch.topk(distances, k, largest=False).indices
        # Extract the labels of the nearest neighbors
        k_neighbor_labels = y_train[k_neighbors]
        # Assign the most common class label to the test sample
        y_pred[i] = torch.mode(k_neighbor_labels).values
    return y_pred

# Convert your numpy arrays to PyTorch tensors
X_train_tensor = torch.tensor(X_train, dtype=torch.float)
X_test_tensor = torch.tensor(X_test, dtype=torch.float)
y_train_tensor = torch.tensor(y_train, dtype=torch.long)

# Define number of neighbors
k = 11

# Perform KNN
y_pred = knn(X_train_tensor, y_train_tensor, X_test_tensor, k)

# y_pred now contains the predicted labels for the test set

# Assuming y_pred contains the predicted labels for the test set
y_test_tensor = torch.tensor(y_test, dtype=torch.long)

# Calculate the number of correct predictions
correct_predictions = torch.sum(y_pred == y_test_tensor)

# Calculate the accuracy
accuracy = correct_predictions.item() / y_test_tensor.size(0)

# Print the accuracy
print(f"Test Accuracy: {accuracy * 100:.2f}%")



"""### Random Forest"""

from sklearn.ensemble import RandomForestClassifier  # or RandomForestRegressor for regression
from sklearn.model_selection import train_test_split


# Initialize the Random Forest model
model = RandomForestClassifier(random_state=42)  # Use RandomForestRegressor for regression

# Train the model on the training data
model.fit(X_train, y_train)

# Make predictions on the test data
y_pred = model.predict(X_test)

# y_pred now contains the predicted labels (for classification) or values (for regression)

# Get feature importances
importances = model.feature_importances_

# Convert the importances into a DataFrame
# feature_importance_df = pd.DataFrame({'Feature': train.columns, 'Importance': importances})

# Sort the DataFrame by importance
# feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

# Display the feature importances
print(importances)

