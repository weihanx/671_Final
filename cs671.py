# -*- coding: utf-8 -*-
"""CS671.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WnDeS8xOkyHfEmhl1PbNxIwvLWpGRjey

## Exploratory Data Analysis
"""

# Commented out IPython magic to ensure Python compatibility.
# %load_ext autoreload
# %autoreload 2
from google.colab import drive
import os
import pandas as pd
import ast
import numpy as np
import sys
import pandas as pd

import matplotlib.pyplot as plt
import seaborn as sns
drive.mount("/content/drive")

GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = "CS671FINAL"
GOOGLE_DRIVE_PATH = os.path.join("drive", "My Drive", GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)
print(os.listdir(GOOGLE_DRIVE_PATH))
sys.path.append(GOOGLE_DRIVE_PATH)

"""### Raw Data Analysis"""

train_file_path = os.path.join(GOOGLE_DRIVE_PATH, 'train.csv')
train = pd.read_csv(train_file_path) # don't read the first unnamed column

print(f"train shape = {train.shape}")
print(f"There are {len(train.columns)} columns in training dataset. The column names = {train.columns}") # 47

numeric_cols = train.select_dtypes(include=[np.number])
data_types = train.dtypes
object_cols = train.select_dtypes(include=['object'])

print(f"There are {len(numeric_cols.columns)} numerical columns. They are {numeric_cols.columns}") # 28
print(f"There are {len(object_cols.columns)} object columns. They are {object_cols.columns}") # 28

# Assuming 'df' is your DataFrame
na_rows_count = train.isna().any(axis=1).sum()
print(f"There are {na_rows_count} rows in training dataset having at least one NA value.")

test_file_path = os.path.join(GOOGLE_DRIVE_PATH, 'test.csv')
test = pd.read_csv(test_file_path) # don't read the first unnamed column

print(f"Test Shape = {test.shape}")
print(f"There are {len(test.columns)} columns in test dataset. The column names = {train.columns}") # 47

"""###### Pick Selected Features from train_sentiment.csv saved at train_selected.csv"""

feature_columns = ['id','neighbourhood_group_cleansed', 'latitude', 'longitude', 'host_id', 'host_is_superhost', 'host_listings_count', 'host_total_listings_count',
                   'host_has_profile_pic', 'host_identity_verified', 'accommodates', 'bathrooms_text', 'beds', 'amenities', 'minimum_nights', 'maximum_nights', 'minimum_minimum_nights',
                   'maximum_minimum_nights', 'minimum_maximum_nights', 'maximum_maximum_nights', 'minimum_nights_avg_ntm', 'maximum_nights_avg_ntm', 'has_availability', 'availability_30', 'availability_60','availability_90', 'availability_365',
                   'number_of_reviews', 'number_of_reviews_ltm', 'number_of_reviews_l30d', 'calculated_host_listings_count', 'calculated_host_listings_count_entire_homes',
           'calculated_host_listings_count_private_rooms', 'calculated_host_listings_count_shared_rooms', 'Sentiment', 'instant_bookable', 'room_type', 'property_type', 'host_verifications', 'host_since','price', 'description']
feature_columns_test =  ['id','neighbourhood_group_cleansed', 'latitude', 'longitude', 'host_id', 'host_is_superhost', 'host_listings_count', 'host_total_listings_count',
                   'host_has_profile_pic', 'host_identity_verified', 'accommodates', 'bathrooms_text', 'beds', 'amenities', 'minimum_nights', 'maximum_nights', 'minimum_minimum_nights',
                   'maximum_minimum_nights', 'minimum_maximum_nights', 'maximum_maximum_nights', 'minimum_nights_avg_ntm', 'maximum_nights_avg_ntm', 'has_availability', 'availability_30', 'availability_60','availability_90', 'availability_365',
                   'number_of_reviews', 'number_of_reviews_ltm', 'number_of_reviews_l30d', 'calculated_host_listings_count', 'calculated_host_listings_count_entire_homes',
           'calculated_host_listings_count_private_rooms', 'calculated_host_listings_count_shared_rooms', 'Sentiment', 'instant_bookable', 'room_type', 'property_type', 'host_verifications','host_since', 'description']

print(f"len of feature columns of training = {len(feature_columns)}")

train_file_path = os.path.join(GOOGLE_DRIVE_PATH, 'train_sentiment.csv')
train = pd.read_csv(train_file_path) # don't read the first unnamed column
na_rows_count = train.isna().any(axis=1).sum()
print(f"There are {na_rows_count} rows in training dataset having at least one NA value.")
selected_columns =train[feature_columns]
selected_columns.to_csv('/content/drive/My Drive/CS671FINAL/train_selected_des.csv', index=False) # keep id column


test_file_path = os.path.join("/content/drive/My Drive/CS671FINAL", 'test_sentiment.csv')
test = pd.read_csv(test_file_path) # don't read the first unnamed column
selected_columns =test[feature_columns_test]
na_rows_count = train.isna().any(axis=1).sum()
print(f"There are {na_rows_count} rows in training dataset having at least one NA value.")
selected_columns.to_csv('/content/drive/My Drive/CS671FINAL/test_selected_des.csv', index=False) # keep id column

"""### Dataset Processing"""

# import pandas as pd

# # Replace 'path_to_csv' with your actual file path
train_file_path = os.path.join(GOOGLE_DRIVE_PATH, 'train_selected.csv')
test_file_path = os.path.join(GOOGLE_DRIVE_PATH, 'test_selected.csv')

# # Load the CSV file into a DataFrame
train = pd.read_csv('/content/drive/My Drive/CS671FINAL/train_selected_des.csv') # don't read the first unnamed column
test = pd.read_csv('/content/drive/My Drive/CS671FINAL/test_selected_des.csv')
print(f"shape of train file: {train.shape}")

print(f"shape of test file: {test.shape}") # for prediction
na_rows_count = train.isna().any(axis=1).sum()
print(f"There are {na_rows_count} rows in training dataset having at least one NA value.")

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

df = train
df_test = test

feature_columns = ['neighbourhood_group_cleansed', 'latitude', 'longitude', 'host_id', 'host_is_superhost', 'host_listings_count', 'host_total_listings_count',
                   'host_has_profile_pic', 'host_identity_verified', 'accommodates', 'bathrooms_text', 'beds', 'amenities', 'minimum_nights', 'maximum_nights', 'minimum_minimum_nights',
                   'maximum_minimum_nights', 'minimum_maximum_nights', 'maximum_maximum_nights', 'minimum_nights_avg_ntm', 'maximum_nights_avg_ntm', 'has_availability', 'availability_30', 'availability_60','availability_90', 'availability_365',
                   'number_of_reviews', 'number_of_reviews_ltm', 'number_of_reviews_l30d', 'calculated_host_listings_count', 'calculated_host_listings_count_entire_homes',
           'calculated_host_listings_count_private_rooms', 'calculated_host_listings_count_shared_rooms', 'Sentiment', 'instant_bookable', 'room_type', 'property_type', 'host_verifications', 'host_since', 'price', 'description']



tf_features = ['host_has_profile_pic', 'host_identity_verified', 'has_availability', 'instant_bookable', 'host_is_superhost']
category_features = ['neighbourhood_group_cleansed', 'room_type', 'host_verifications']
text_features = ['amenities', 'property_type', 'bathrooms_text', 'Sentiment','host_since', 'description']
target =['price']
test_features = [f for f in feature_columns if f not in target]
numerical_features = [f for f in feature_columns if f not in (tf_features + category_features + text_features+target)]
print(f"length of numerical featrues = {len(numerical_features)}")

"""#### Distribution of numerical value

"""

import pandas as pd
import matplotlib.pyplot as plt

# Assuming df is your DataFrame
# Replace these column names with the columns you want to plot
columns_to_plot = numerical_features

# Calculating means and standard deviations
means = [df[col].mean() for col in columns_to_plot]
stds = [df[col].std() for col in columns_to_plot]

import matplotlib.pyplot as plt

# Assuming 'means' and 'stds' are your calculated mean and standard deviation lists
# and 'columns_to_plot' is your list of column names

plt.figure(figsize=(20, 10))  # Adjust the figure size as necessary
bar_positions = range(len(columns_to_plot))
plt.bar(bar_positions, means, yerr=stds, color='skyblue', capsize=5, error_kw={'elinewidth':2, 'ecolor':'black'})

# Rotate x-axis labels for better readability
plt.xticks(bar_positions, columns_to_plot, rotation=45, ha='right', fontsize=12)

# If the range of values is very large, using a logarithmic scale can be helpful
plt.yscale('log')

# Adding labels, title, and grid
plt.xlabel('Columns', fontsize=14)
plt.ylabel('Mean Values', fontsize=14)
plt.title('Mean and Standard Deviation of Numerical Columns', fontsize=16)
plt.grid(True)

# Show the plot
plt.tight_layout()  # Adjust the padding between and around subplots
plt.show()
plt.savefig("distribution")

"""#### Plot the Distribution of Target Label

"""

import matplotlib.pyplot as plt
import pandas as pd

# Assuming df is your DataFrame and it has a column named 'price'
# df = pd.DataFrame(...)  # your DataFrame loading/creation here

# Plotting
plt.figure(figsize=(10, 6))  # Adjust the size of the figure as needed
plt.hist(df['price'], bins=20, edgecolor='black')  # You can adjust the number of bins
plt.title('Histogram of Prices')
plt.xlabel('Price')
plt.ylabel('Frequency')
plt.grid(True)  # Adds a grid for better readability
plt.show()

"""#### Categorical Variables:"""

"""
true or false --> 0 or 1
"""
# df[tf_features] = df[tf_features].astype(int)
import pandas as pd
tf_features = ['host_has_profile_pic', 'host_identity_verified', 'has_availability', 'instant_bookable', 'host_is_superhost']

# Define a mapping dictionary
mapping = {'t': 1, 'f': 0}

# Apply the mapping to the desired columns
df['host_has_profile_pic'] = df['host_has_profile_pic'].astype(str).map(mapping)
df['host_identity_verified'] = df['host_identity_verified'].astype(str).map(mapping)
df['has_availability'] = df['has_availability'].astype(str).map(mapping)
df['instant_bookable'] = df['instant_bookable'].astype(str).map(mapping)
df['host_is_superhost'] = df['host_is_superhost'].astype(str).map(mapping)

df_test['host_has_profile_pic'] = df_test['host_has_profile_pic'].astype(str).map(mapping)
df_test['host_identity_verified'] = df_test['host_identity_verified'].astype(str).map(mapping)
df_test['has_availability'] = df_test['has_availability'].astype(str).map(mapping)
df_test['instant_bookable'] = df_test['instant_bookable'].astype(str).map(mapping)
df_test['host_is_superhost'] = df_test['host_is_superhost'].astype(str).map(mapping)

# The DataFrame after conversion
print(df.columns)
print(len(df.columns))

"""
multilabel --> one hot encoding; includ newly added
"""
df = pd.get_dummies(df, columns=['room_type'])
df = pd.get_dummies(df, columns=['Sentiment'])
# neighbourhood_group_cleansed
df = pd.get_dummies(df, columns=['neighbourhood_group_cleansed'])

df_test = pd.get_dummies(df_test, columns=['room_type'])
df_test = pd.get_dummies(df_test, columns=['Sentiment'])
# neighbourhood_group_cleansed
df_test = pd.get_dummies(df_test, columns=['neighbourhood_group_cleansed'])

# Drop Sentiment = -1: because I assign -1 when no sentiment can be predicted
df.drop('Sentiment_-1', axis=1, inplace=True)
df_test.drop('Sentiment_-1', axis=1, inplace=True)
print(df.columns)
print(len(df.columns))

"""
Multi-label: host_verification
"""
df['host_verifications'] = df['host_verifications'].apply(ast.literal_eval)

# Identify all unique labels
all_labels = sorted(set(label for row in df['host_verifications'] for label in row))

# Create a mapping of labels to indices
label_to_index = {label: idx for idx, label in enumerate(all_labels)}

# One-hot encode each row and append the result to the DataFrame
for label in all_labels:
    # Create a new column for each label, setting to 1 where the label is present in the row, else 0
    df[f'host_verification_{label}'] = df['host_verifications'].apply(lambda x: int(label in x))

# Now df has the original 'host_verifications' column and additional columns for the one-hot encoded features
# print(df.columns)
df.drop('host_verifications', axis=1, inplace=True)


df_test['host_verifications'] = df_test['host_verifications'].apply(ast.literal_eval)

# Identify all unique labels
all_labels = sorted(set(label for row in df_test['host_verifications'] for label in row))

# Create a mapping of labels to indices
label_to_index = {label: idx for idx, label in enumerate(all_labels)}

# One-hot encode each row and append the result to the DataFrame
for label in all_labels:
    # Create a new column for each label, setting to 1 where the label is present in the row, else 0
    df_test[f'host_verification_{label}'] = df_test['host_verifications'].apply(lambda x: int(label in x))

# Now df has the original 'host_verifications' column and additional columns for the one-hot encoded features
print(df_test.columns)
df_test.drop('host_verifications', axis=1, inplace=True)

"""###### Unique class in categorical variable"""

"""
Decide to treat as text or one hot variables
"""
# Assuming df is your DataFrame
combined_list = []
# deal with property type:
unique_values_property = df['property_type'].unique()
# deal with room type:
unique_values_room = df['room_type'].unique()

unique_values_neigh_group = df['neighbourhood_group_cleansed'].unique()
print(f"number of unique group = {len(unique_values_neigh_group)}")
print(f"number of unique property = {len(unique_values_property)}, number of unique room = {len(unique_values_room)}")

"""#### Text"""

"""
Time: host since
"""
df['host_since'] = pd.to_datetime(df['host_since'])

# Calculate the difference in days between today and the date column
df['days_difference'] = (pd.Timestamp.today() - df['host_since']).dt.days

df.drop('host_since', axis=1, inplace=True)
print(df['days_difference'].head())
print(df.columns)

df_test['host_since'] = pd.to_datetime(df_test['host_since'])

# Calculate the difference in days between today and the date column
df_test['days_difference'] = (pd.Timestamp.today() - df_test['host_since']).dt.days

df_test.drop('host_since', axis=1, inplace=True)
print(df_test['days_difference'].head())
print(df_test.columns)

"""
text: amenities: transform, no need for noramlize, default is l2 norm
"""

from sklearn.feature_extraction.text import TfidfVectorizer


import pandas as pd

# check NA type for amenities
missing_values = df['amenities'].isna().any()
print(f"missing values = {missing_values}")

vectorizer = TfidfVectorizer(max_features = 50, stop_words='english')
text_features_df = vectorizer.fit_transform(df['amenities'])


text_features_df = pd.DataFrame(text_features_df.toarray(),columns=vectorizer.get_feature_names_out())
df= pd.concat([df.drop('amenities', axis=1), text_features_df], axis=1) # expand more columns

print(df.columns)


# Do not refit on the test data
text_features_test = vectorizer.transform(df_test['amenities'])

text_features_df_test = pd.DataFrame(text_features_test.toarray(), columns=vectorizer.get_feature_names_out()) # transfer back to panda dataframe
df_test= pd.concat([df_test.drop('amenities', axis=1), text_features_df_test], axis=1) # expand more columns
print(df_test.columns)

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import StandardScaler
import pandas as pd

# check NA value for property_type
missing_values = df['property_type'].isna().any()
print(f"missing values = {missing_values}")

vectorizer = TfidfVectorizer(max_features=50, stop_words='english')
text_features = vectorizer.fit_transform(df['property_type'])

scaled_text_features_df = pd.DataFrame(text_features.toarray(), columns=vectorizer.get_feature_names_out())
df = pd.concat([df.drop('property_type', axis=1), scaled_text_features_df], axis=1)


text_features_test = vectorizer.transform(df_test['property_type'])

text_features_df_test = pd.DataFrame(text_features_test.toarray(), columns=vectorizer.get_feature_names_out())
df_test = pd.concat([df_test.drop('property_type', axis=1), text_features_df_test], axis=1)

# import pandas as pd
# import numpy as np
# import torch
# import torchtext
glove = torchtext.vocab.GloVe(name='6B', dim=100)  # For example, using 100-dimensional embeddings

def get_text_embedding(text, glove_vectors):
    text = str(text)
    tokens = text.lower().split()
    embeddings = [glove_vectors[word] for word in tokens if word in glove_vectors.stoi]
    if embeddings:
        embeddings = torch.stack(embeddings)
        avg_embedding = embeddings.mean(dim=0).numpy()
    else:
        avg_embedding = np.zeros(glove_vectors.dim)  # Return a zero vector if text is empty or all words are out-of-vocabulary

    return avg_embedding

# Applying the function and expanding the results into separate columns
for i in range(glove.dim):
    df[f'text_emb_{i+1}'] = df['description'].apply(lambda x: get_text_embedding(x, glove)[i])

# import pandas as pd
# import numpy as np
# import torch
# import torchtext
# glove = torchtext.vocab.GloVe(name='6B', dim=100)  # For example, using 100-dimensional embeddings

def get_text_embedding(text, glove_vectors):
    text = str(text)
    tokens = text.lower().split()
    embeddings = [glove_vectors[word] for word in tokens if word in glove_vectors.stoi]
    if embeddings:
        embeddings = torch.stack(embeddings)
        avg_embedding = embeddings.mean(dim=0).numpy()
    else:
        avg_embedding = np.zeros(glove_vectors.dim)  # Return a zero vector if text is empty or all words are out-of-vocabulary

    return avg_embedding

# Applying the function and expanding the results into separate columns
for i in range(glove.dim):
    df_test[f'text_emb_{i+1}'] = df_test['description'].apply(lambda x: get_text_embedding(x, glove)[i])

# # tokenize the text of bath
"""
text: bath
"""
# ! pip install nltk # for tokenize
import pandas as pd
import nltk

# nltk.download('punkt')
# Ensure you have the necessary tokenizer downloaded

def tokenize_text(text):
    # Check if the text is None or not
    if text is None:
        print("Text is None, returning 'N/A'.")
        return None
    try:
        # Now we can be sure that text is not None and try to tokenize it
        # print(f"text = {text.split()}")
        return text.split()
    except Exception as e:
        print(f"Text cannot be tokenized: {text} - Error: {e}")
        return None


# na_rows_count = df['bathrooms_text'].isna().any()
# print(f"There are {na_rows_count} rows in training dataset having at least one NA value.")

# Drop the rows where df['bathrooms_text'] = None. Only one row

# Apply the function to each item in the column, assuming 'df' is your DataFrame
df['bathrooms_text'] =  df['bathrooms_text'].astype(str) # important, expect a string
# na_rows_count = df['bathrooms_text'].isna().any()
# print(f"There are {na_rows_count} rows in training dataset having at least one NA value.")

df['bathrooms_text'] = df['bathrooms_text'].apply(tokenize_text)
# df_test['bathrooms_text'] =  df_test['bathrooms_text'].astype(str) # important, expect a string
# df_test['bathrooms_text'] = df_test['bathrooms_text'].apply(tokenize_text)

def conversion(text_str):
  need_division = False

  if "shared" in text_str:
    need_division = True

  for i in range(len(text_str)):
    try:
      float(text_str[i])
      is_number = True
    except ValueError:
      is_number = False

    if is_number == True: # if it is digit
      # print(f"text str = {text_str[i]}")
      if need_division:
        res = float(text_str[i])/2
        # print(f"res /2 = {res}")
        return res # split if shared
      else:
        res = float(text_str[i])
        # print(f"res = {res}")
        return res # don't split
    else:
      return 0 # no bath room
df['bathrooms_text'] = df['bathrooms_text'].apply(conversion)

df_test['bathrooms_text'] = df_test['bathrooms_text'].apply(conversion)

print(f"len = {len(df.columns)}")
print(f"len = {len(df_test.columns)}")

"""
noralize on numerical data; include newly added
"""

from sklearn.preprocessing import StandardScaler
import pandas as pd


# Assuming df is your DataFrame and numerical_features is your list of numerical columns
scaler = StandardScaler()
# Standard scaler is sensitive to NA vlaues, drop those rows with NA values

na_rows_count = df.isna().any(axis=1).sum()
print(f"There are {na_rows_count} rows in training dataset having at least one NA value.")


numerical_features = numerical_features + ['days_difference'] + ['bathrooms_text']
print(df.columns)

df= df.dropna(subset=numerical_features)

print(f"After dealing with NA values, my dataframe has shape = {df.shape}") # probably need those processed

# Scale the numerical features and replace them in the DataFrame
df[numerical_features] = scaler.fit_transform(df[numerical_features])

# fit the mean and value of the train set to the test set
df_test[numerical_features] = scaler.transform(df_test[numerical_features])

"""Saved processed csv as train_processed.csv"""

df.to_csv(os.path.join(GOOGLE_DRIVE_PATH, 'train_processed_des_bert.csv'), index=False)
df_test.to_csv(os.path.join(GOOGLE_DRIVE_PATH, 'test_processed_des_bert.csv'), index=False)

"""###### Load Pretained Model and Perform Sentimental Analysis on it \\
Distilbert-base-uncased-emotion is a model fine-tuned for detecting emotions in texts, including sadness, joy, love, anger, fear and surprise\\
Saved at train_sentiment.csv

"""

# !pip install -q transformers
# from transformers import pipeline

# # Initialize sentiment analysis pipeline
# sentiment_pipeline = pipeline("text-classification",model='bhadresh-savani/distilbert-base-uncased-emotion', return_all_scores=True)
# sentiment_list = ['sadness', 'joy', 'love', 'anger', 'fear', 'surprise']
# # Function to get sentiment
# def get_sentiment(text):
#   # print(f"type of text = {type(text)}")
#   if type(text) != str:
#     print(f"error input, should be all zeros")
#     return -1
#   try:
#     prediction = sentiment_pipeline(text) # should be a dictionary
#     emotion_scores = {}
#     # print(f"column")
#     for i in range(5):
#       emotion_scores[prediction[0][i]['label']] = prediction[0][i]['score']
#       max_emotion = max(emotion_scores, key=emotion_scores.get) # get the emotion with the highest score, emotion is a nnumber

#     # one_hot_vector = [1 if emotion == max_emotion else 0 for emotion in emotion_scores]
#     # index = sentiment_list.index(str(max_emotion))
#     # print(f"max_emotion = {index}") # one prediction should have one max emotion one --> checked one row one prediction
#     return str(max_emotion)
#   except:
#     print(f"error input, should be all zeros")
#     return -1
#   # prediction = sentiment_pipeline(text) # should be a dictionary
#   # res = sentiment_pipeline(text) # should be a dictionary
#   # print(f"res = {res}")


# # Apply sentiment analysis to the description column
# test['Sentiment'] = test['description'].apply(get_sentiment)

# # Save the dataframe back to CSV
# test.to_csv('test_sentiment.csv', index=False) #

# # saved at google colab

"""### Data Split

#### Load Processed Data
"""

train_file_path = os.path.join(GOOGLE_DRIVE_PATH, 'train_processed_des_glove.csv')
train = pd.read_csv(train_file_path) # don't read the first unnamed column
print(train.shape)

test_file_path = os.path.join(GOOGLE_DRIVE_PATH, 'test_processed_des_glove.csv')
test = pd.read_csv(test_file_path) # don't read the first unnamed column

print(test.shape)

train.drop('description', axis=1, inplace=True)
test.drop('description', axis=1, inplace=True)

# Separate the target variable ('price')
y = train['price'].values.astype(int)

# Drop the 'price' column to create the features DataFrame and then get its values
X = train.drop('price', axis=1).values

## decide the weight assign to different class
import numpy as np

# Count the frequency of each class
class_counts = np.bincount(y.astype(int))

# Calculate class weights
class_weights = 1. / class_counts

# Normalize the weights so that they sum to 1
class_weights = class_weights / np.sum(class_weights)
# print(f"class weights = {class_weights}")
# Create a dictionary mapping class labels to weights
weight_dict_non = {i: weight for i, weight in enumerate(class_weights)}
print(weight_dict_non)

true_test = test.values
print(f"true test shape = {test.shape}")

# Split the data into training and test sets (80% train, 20% test)
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
print(f"X_train.shape = {X_train.shape}")
print(f"X_test.shape = {X_test.shape}")

train_filtered = train.dropna()
# train_filtered.drop('description', axis=1, inplace=True)
y_filtered = train_filtered['price'].values

# Drop the 'price' column to create the features DataFrame and then get its values
X_filtered = train_filtered.drop('price', axis=1).values

## decide the weight assign to different class
import numpy as np

# Count the frequency of each class
class_counts = np.bincount(y_filtered.astype(int))

# Calculate class weights
class_weights = 1. / class_counts

# Normalize the weights so that they sum to 1
class_weights = class_weights / np.sum(class_weights)
# print(f"class weights = {class_weights}")
# Create a dictionary mapping class labels to weights
weight_dict_filtered = {i: weight for i, weight in enumerate(class_weights)}
print(weight_dict_filtered)

# Perform PCA on filteed
from sklearn.decomposition import PCA

pca = PCA(n_components=100)
pca.fit(X_filtered)
X_filtered = pca.transform(X_filtered)

true_pca = pca.transform(true_test)
print(f"shape of output of PCA = {X_filtered.shape}")
print(f"shape of output of PCA = {true_pca.shape}")

# Split the data into training and test sets (80% train, 20% test)
from sklearn.model_selection import train_test_split
X_train_filtered, X_test_filtered, y_train_filtered, y_test_filtered = train_test_split(X_filtered, y_filtered, test_size=0.2, random_state=42)
print(f"X_train.shape = {X_train_filtered.shape}")
print(f"X_test.shape = {X_test_filtered.shape}")

"""###### Distribution of Important Variable"""

import matplotlib.pyplot as plt
import seaborn as sns

# List of columns you want to plot
columns_to_plot = ['accommodates', 'bathrooms_text', 'room_type_Entire home/apt', 'room' ]

# Number of rows/columns for the subplot grid
num_rows = 2
num_cols = 2  # Adjust as needed

# Set the style of seaborn for better aesthetics
sns.set(style='whitegrid')

# Create a figure with subplots
plt.figure(figsize=(12, 10))  # Adjust size as needed

for i, column in enumerate(columns_to_plot):
    plt.subplot(num_rows, num_cols, i + 1)
    sns.histplot(train[column], color='blue', label='Training Set', kde=True)
    sns.histplot(test[column], color='red', label='Test Set', kde=True)
    plt.title(f'Distribution of {column}')
    plt.xlabel(column)
    plt.ylabel('Frequency')
    plt.legend()

plt.tight_layout()
plt.show()

"""### Models

### XGBoost
Define my problem: multiclass classification
"""

!pip install xgboost
import xgboost as xgb

# record time
import time
start_time = time.time()

import xgboost as xgb
from sklearn.metrics import accuracy_score, confusion_matrix

# Your XGBoost classifier object
xg_clf = xgb.XGBClassifier(
    n_estimators=1000,
    learning_rate=0.1,
    max_depth=9,
    min_child_weight=3,
)

# Generate sample weights based on the class of each sample
sample_weights = np.array([weight_dict[class_label] for class_label in y_train])

# Fit the classifier to the training set with sample weights
xg_clf.fit(X_train, y_train, sample_weight=sample_weights)

# Predict on the test set


# Compute the confusion matrix
# cm = confusion_matrix(y_test, y_pred)

# print(f"confusion matrix is = {cm}")

y_pred = xg_clf.predict(X_test)

from sklearn.metrics import classification_report
print(classification_report(y_test, y_pred))

from sklearn.metrics import classification_report
print(classification_report(y_test, y_pred))

from sklearn.metrics import accuracy_score

# Assuming y_pred contains the predicted labels for the test set
test_accuracy = accuracy_score(y_test_filtered, y_pred)

# Print the test accuracy
print(f"Test Accuracy: {test_accuracy * 100:.2f}%")

# sample_weight: Array of weights that are assigned to individual samples.
from sklearn.metrics import balanced_accuracy_score
balanced_accuracy = balanced_accuracy_score(y_test, y_pred)
print(f"balanced accuracy = {balanced_accuracy}")

import xgboost as xgb
from sklearn.metrics import accuracy_score, confusion_matrix

# Your XGBoost classifier object
xg_clf = xgb.XGBClassifier(
    n_estimators=1000,
    learning_rate=0.1,
    max_depth=20,
    min_child_weight=1,
    # other parameters
)

# Generate sample weights based on the class of each sample
sample_weights = np.array([weight_dict[class_label] for class_label in y_train_filtered])

# Fit the classifier to the training set with sample weights
xg_clf.fit(X_train_filtered, y_train_filtered, sample_weight=sample_weights)

# Predict on the test set
y_pred = xg_clf.predict(X_test_filtered)

# Compute the confusion matrix
cm = confusion_matrix(y_test_filtered, y_pred)

print(f"confusion matrix is = {cm}")

from sklearn.metrics import classification_report
print(classification_report(y_test, y_pred))

from sklearn.metrics import accuracy_score

# Assuming y_pred contains the predicted labels for the test set
test_accuracy = accuracy_score(y_test, y_pred)

# Print the test accuracy
print(f"Test Accuracy: {test_accuracy * 100:.2f}%")



"""##### Hyperparameter Searching

eta: learning rate: 0.01-0.2: choose a relative high lr



min_child_weight: tune with cv

max_depth 3-10

gamma: specifies the minimum loss reduction required to make a split, depending on loss function

lambda: L2 regularization

alpha: L1 regulariztiaon

colsample_bytree: denotes the fraction of observations to be random samples for each tree: 0.5-1
"""

import matplotlib.pyplot as plt
import numpy as np
from sklearn.model_selection import cross_val_score
# Assuming X_train, y_train are defined
# Define the parameter grid
param_grid = {
    'max_depth': [6, 9],
    'min_child_weight': [1, 3, 5]
}

# Initialize dictionary to store results
results = {}

# Loop through parameter combinations
for max_depth in param_grid['max_depth']:
    for child_weight in param_grid['min_child_weight']:
        # Create XGBoost model with current parameters
        xgb_model = xgb.XGBClassifier(max_depth=max_depth, min_child_weight=child_weight,
                                      learning_rate=0.1, n_estimators=1000)

        # Perform cross-validation and calculate mean accuracy
        scores = cross_val_score(xgb_model, X_train, y_train, cv=5, scoring='balanced_accuracy')
        mean_accuracy = np.mean(scores)
        print(f"max_depth = {max_depth}, min_child_weight = {child_weight}, mean_balanced_accuracy= {mean_accuracy}")
        # Store results
        results[(max_depth, child_weight)] = mean_accuracy

# Prepare data for plotting
max_depths, child_weights, accuracies = zip(*[(md, cw, acc) for (md, cw), acc in results.items()])

# Convert to NumPy arrays for ease of manipulation
max_depths = np.array(max_depths)
child_weights = np.array(child_weights)
accuracies = np.array(accuracies)

# Plotting
plt.figure(figsize=(10, 6))
for depth in param_grid['max_depth']:
    indices = max_depths == depth
    plt.plot(child_weights[indices], accuracies[indices], label=f'max_depth={depth}')

plt.xlabel('max_child_weight')
plt.ylabel('Mean Accuracy')
plt.title('Accuracy vs. Hyperparameters')
plt.legend()
plt.grid(True)
plt.show()

param_grid = {
  'gamma':[0.5,0.8,1]
}
for g in param_grid['gamma']:
    # Create XGBoost model with current parameters
    xgb_model = xgb.XGBClassifier(max_depth=9, min_child_weight=3,
                                  learning_rate=0.1, n_estimators=1000, gamma = g)

    # Perform cross-validation and calculate mean accuracy
    scores = cross_val_score(xgb_model, X_train, y_train, cv=5, scoring='balanced_accuracy')
    mean_accuracy = np.mean(scores)
    print(f"gamma = {g}, mean_accuracy = {mean_accuracy}")

from sklearn.model_selection import cross_val_score
param_grid = {
  'alpha':[0.5,0.8,1]
}
for l in param_grid['alpha']:
    # Create XGBoost model with current parameters
    xgb_model = xgb.XGBClassifier(max_depth=9, min_child_weight=3,learning_rate=0.1, alpha = l, n_estimators=1000, gamma = 0.5)

    # Perform cross-validation and calculate mean accuracy
    scores = cross_val_score(xgb_model, X_train, y_train, cv=5, scoring='balanced_accuracy')
    mean_accuracy = np.mean(scores)
    print(f"alpha = {l}, mean_accuracy = {mean_accuracy}")

"""##### Predict on test set"""

# feature importance
import matplotlib.pyplot as plt
import xgboost as xgb

# Assuming xg_clf is your trained XGBoost model
# Get feature importances
feature_importances = xg_clf.feature_importances_
train.drop(['price'], axis=1, inplace=True)
# Get feature names
feature_names = train.columns
# Create a series with feature importances and labels
importance_series = pd.Series(feature_importances, index=[feature_names])

# Sort the feature importances
importance_series_sorted = importance_series.sort_values(ascending=False).head(20)

# Plot
plt.figure(figsize=(10, 6))
importance_series_sorted.plot(kind='bar')
plt.title('Most important 20 features')
plt.xlabel('Features')
plt.ylabel('Importance')
plt.show()

test_pred = xg_clf.predict(true_test)
predictions_df = pd.DataFrame(test_pred, columns=['price'])

# Create a new column for row ID, starting from 0
predictions_df['id'] = range(0, len(predictions_df))

# Reorder the columns to have 'ID' first and 'price' second
predictions_df = predictions_df[['id', 'price']]
print(f"submission = {predictions_df.shape}")
# Save to CSV with the new row ID as the first column
predictions_df.to_csv('/content/drive/My Drive/CS671FINAL/submission.csv', index=False)

"""### Neural Network"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import pandas as pd
import matplotlib.pyplot as plt
from torch.utils.data import Dataset
from torch.utils.data import DataLoader
import numpy as np

# NOT GOOD AT DEAL WITH NA VALUE, NEED TO DROP
# fill the NA value with mode value, we
df = X_train_filtered

"""#### Make custom dataset"""

class CustomDataset(Dataset):
    def __init__(self, features, labels):
        """
        Initialize the dataset with features and labels.
        :param features: A list or array of features (X_train).
        :param labels: A list or array of labels (y_train).
        """
        self.features = features
        self.labels = labels

    def __len__(self):
        """
        Return the total number of samples in the dataset.
        """
        return len(self.labels)

    def __getitem__(self, idx):
        """
        Generate one sample of data.
        :param idx: The index of the sample.
        """
        # Get the sample and its corresponding label
        sample = self.features[idx]
        label = self.labels[idx]

        # Convert to PyTorch tensors
        sample_tensor = torch.tensor(sample, dtype=torch.float32)
        label_tensor = torch.tensor(label, dtype=torch.float32)  # Use torch.long for classification

        return sample_tensor, label_tensor

"""#### Training"""

train_dataset = CustomDataset(X_train_filtered, y_train_filtered.ravel())
valid_dataset = CustomDataset(X_test_filtered, y_test_filtered.ravel())
print(f"train dataset shape = {len(train_dataset)}")
from torch.utils.data import DataLoader
# Parameters
batch_size = 256
# Create the DataLoaders
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)
for data, label in train_loader:
    print(f"data shape = {data.shape}")
    print(f"label shape = {label.shape}")
    break

# training args
num_epochs = 100

## need initialization
# treat description as an embedding layer as it has a brief summary of the facilities and might involve customer's atttitude over the Airbnb

## decide the weight assign to different class, need drop
import numpy as np

# Count the frequency of each class
y_train_filtered = y_train_filtered.astype(int)
class_counts = np.bincount(y_train_filtered)

# Calculate class weights
class_weights = 1. / class_counts

# Normalize the weights so that they sum to 1
class_weights = class_weights / np.sum(class_weights)
print(f"class weights = {class_weights}")
# Create a dictionary mapping class labels to weights
weight_dict = {i: weight for i, weight in enumerate(class_weights)}

print(f"weight_dict = {weight_dict}")
# class_weights = np.array([weight_dict[class_label] for class_label in y_train_filtered])

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from sklearn.metrics import balanced_accuracy_score, classification_report

# Define the network
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.fc1 = nn.Linear(100, 512)
        # self.fc2 = nn.Linear(1024, 512)
        self.fc3 = nn.Linear(512, 256)
        self.fc4 = nn.Linear(256, 128)
        self.fc5 = nn.Linear(128, 6)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        # x = F.relu(self.fc2(x))
        x = F.relu(self.fc3(x))
        x = F.relu(self.fc4(x))
        x = self.fc5(x)  # No softmax needed for CrossEntropyLoss
        return x
    def predict_proba(self, x):
      return F.softmax(self.forward(x), dim=1)

# Initialize the network
net = Net()

# If using CUDA (uncomment if you're using GPU)
net.to("cuda")
# print(f"shape of class weights = {class_weights.shape}")
class_weights = torch.Tensor(class_weights).to("cuda")
# Loss and optimizer
criterion = nn.CrossEntropyLoss(weight=class_weights)
optimizer = optim.Adam(net.parameters(), lr=0.001, weight_decay = 1e-5) # add L2 regualriztiaon

# Training loop
epochs = 100
import time
start_time = time.time()

for epoch in range(epochs):
    total_loss = 0
    for data, label in train_loader:
        # Flatten data and transfer to CUDA if necessary
        data = data.view(-1, 100)  # Reshape data
        label = label.long()
        # if using CUDA (uncomment if you're using GPU)
        data, label = data.to("cuda"), label.to("cuda")

        optimizer.zero_grad()
        outputs = net(data)
        loss = criterion(outputs, label)
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    average_loss = total_loss / len(train_loader)
    print(f'Epoch {epoch+1}, Average Training Loss: {average_loss}')

    with torch.no_grad():
        total_loss = 0
        total_correct = 0
        total_samples = 0
        all_predictions = []
        all_true_labels = []

        for data, label in valid_loader:
            data = data.view(-1, 100)  # Reshape data
            data, label = data.to("cuda"), label.to("cuda")
            label = label.long()
            outputs = net(data)
            loss = criterion(outputs, label)
            total_loss += loss.item()

            _, predicted = torch.max(outputs.data, 1)
            total_correct += (predicted == label).sum().item()
            total_samples += label.size(0)

            all_predictions.extend(predicted.view(-1).cpu().numpy())
            all_true_labels.extend(label.view(-1).cpu().numpy())

        average_loss = total_loss / len(valid_loader)
        accuracy = 100 * total_correct / total_samples
        print(f'Average Validation Loss: {average_loss}')

        # Calculate and print balanced accuracy
        balanced_acc = balanced_accuracy_score(all_true_labels, all_predictions)
        print(f'Balanced Accuracy: {balanced_acc}')

        # Print classification report
        # print(classification_report(all_true_labels, all_predictions))
    net.train()  # Set the model back to training mode
time_elapsed = time.time() - start_time
print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')

import matplotlib.pyplot as plt

# Data
x = [100, 150, 200]
y = [0.4732, 0.4920, 0.4986]

# Creating the plot
plt.plot(x, y, marker='o')  # 'o' denotes circular markers for each data point

# Adding title and labels
plt.title('Neural Network Hyperparameter Searching')
plt.xlabel('# components')
plt.ylabel('Balance Accuracy')

# Display the plot
plt.show()

print(classification_report(all_true_labels, all_predictions))

"""### XGBoost and MLP Ensemble Learning

Combine the result from XGBoost and MLP

MLP: Return Probabilities for Each Class

### Ensemble on test data.
"""

## Further Split on my test data
X_train_filtered_sub, X_test_filtered_sub, y_train_filtered_sub, y_test_filtered_sub = train_test_split(X_test_filtered, y_test_filtered, test_size=0.2, random_state=42)
print(f"X_train_filtered_sub = {X_train_filtered_sub.shape}")

# XGBoost Result
# ## Further Split on my test data
# X_train_filtered_sub, X_test_filtered_sub, y_train_filtered_sub, y_test_filtered_sub = train_test_split(X_test_filtered, y_test_filtered, test_size=0.2, random_state=42)
# print(f"X_train_filtered_sub = {X_train_filtered_sub.shape}")
# test_pred_xgb = xg_clf.predict_proba(X_train_filtered_sub)
# X_train_filtered_sub = torch.tensor(X_train_filtered_sub).float().to('cuda')
# test_pred_nn = net.predict_proba(X_train_filtered_sub).detach().cpu().numpy()
# meta_features = np.column_stack((test_pred_xgb, test_pred_nn))

# test_pred_xgb_sub = xg_clf.predict_proba(X_test_filtered_sub.cpu().numpy())
# X_test_filtered_sub = torch.tensor(X_test_filtered_sub).float().to('cuda')
# test_pred_nn_sub = net.predict_proba(X_test_filtered_sub).detach().cpu().numpy()
# meta_features_test = np.column_stack((test_pred_xgb_sub, test_pred_nn_sub))

test_pred_xgb = xg_clf.predict_proba(X_train_filtered)
X_train_filtered = torch.tensor(X_train_filtered).float().to('cuda')
test_pred_nn = net.predict_proba(X_train_filtered)
meta_features = np.column_stack((test_pred_xgb, test_pred_nn.detach().cpu().numpy()))
#

# Train a meta-classifier on the predictions
from sklearn.linear_model import LogisticRegression

meta_classifier = LogisticRegression()
meta_classifier.fit(meta_features, y_train_filtered)



y_pred = meta_classifier.predict(meta_features_test)

from sklearn.metrics import balanced_accuracy_score
balanced_accuracy = balanced_accuracy_score(y_test_filtered_sub, y_pred)
print(f"balanced accuracy = {balanced_accuracy}")

# x = [0.5381082255832381, 0.5433168463861818]
import matplotlib.pyplot as plt

# Data
y = [0.5381082255832381, 0.5433168463861818]
labels = ['Model A', 'Model B']

# Creating the histogram
plt.bar(labels, y)

# Adding titles and labels
plt.xlabel('Models')
plt.ylabel('Balanced Accuracy')
plt.title('Balanced Accuracy of Models A and B')

# Show the plot
plt.show()

# Predictions on test set
true_test_tensor = torch.tensor(true_pca).float().to('cuda')
test_pred_xgb = xg_clf.predict_proba(true_test_tensor.detach().cpu().numpy())
test_pred_nn = net.predict_proba(true_test_tensor)  # Convert true_test to tensor if necessary

# Convert to numpy and stack
test_pred_xgb_np = test_pred_xgb
test_pred_nn_np = test_pred_nn.detach().cpu().numpy()  # Assuming test_pred_nn is a PyTorch tensor
test_meta_features = np.column_stack([test_pred_xgb_np, test_pred_nn_np])

# Final predictions
final_predictions = meta_classifier.predict(test_meta_features)

predictions_df = pd.DataFrame(final_predictions, columns=['price'])

# Create a new column for row ID, starting from 0
predictions_df['id'] = range(0, len(predictions_df))

# Reorder the columns to have 'ID' first and 'price' second
predictions_df = predictions_df[['id', 'price']]
print(f"submission = {predictions_df.shape}")
# Save to CSV with the new row ID as the first column
predictions_df.to_csv('/content/drive/My Drive/CS671FINAL/submission.csv', index=False)

